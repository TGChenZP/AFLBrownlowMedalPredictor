{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Tune OLS GLM - with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class JiXi:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialise class \"\"\"\n",
    "        self._initialise_objects()\n",
    "\n",
    "        print('JiXi Initialised')\n",
    "\n",
    "\n",
    "\n",
    "    def _initialise_objects(self):\n",
    "        \"\"\" Helper to initialise objects \"\"\"\n",
    "\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.val_x = None\n",
    "        self.val_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.tuning_result = None\n",
    "        self.model = None\n",
    "        self.parameter_choices = None\n",
    "        self.hyperparameters = None\n",
    "        self.checked = None\n",
    "        self.result = None\n",
    "        self.tuning_result_saving_address = None\n",
    "        self.object_saving_address = None\n",
    "        self._up_to = 0\n",
    "        self._seed = 19421221\n",
    "        self.best_score = -np.inf\n",
    "        self.best_combo = None\n",
    "        self.best_clf = None\n",
    "        self.clf_type = None\n",
    "        self.combos = None\n",
    "        self.n_items = None\n",
    "        self.outmost_layer = None\n",
    "        self._core = None\n",
    "        self._relative_combos = None\n",
    "        self._both_combos = None\n",
    "        self._dealt_with = None\n",
    "        self._pos_neg_combos = None\n",
    "        self._abs_max = None\n",
    "        self._new_combos = None\n",
    "        self._parameter_value_map_index = None\n",
    "\n",
    "        self.regression_extra_output_columns = ['Train r2', 'Val r2', 'Test r2', \n",
    "            'Train RMSE', 'Val RMSE', 'Test RMSE', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Time']\n",
    "        self.classification_extra_output_columns = ['Train accu', 'Val accu', 'Test accu', \n",
    "            'Train balanced_accu', 'Val balanced_accu', 'Test balanced_accu', 'Train f1', 'Val f1', 'Test f1', \n",
    "            'Train precision', 'Val precision', 'Test precision', 'Train recall', 'Val recall', 'Test recall', 'Time']\n",
    "\n",
    "        \n",
    "\n",
    "    def read_in_data(self, train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "        \"\"\" Reads in train validate test data for tuning \"\"\"\n",
    "\n",
    "        self.train_x = train_x\n",
    "        print(\"Read in Train X data\")\n",
    "\n",
    "        self.train_y = train_y\n",
    "        print(\"Read in Train x data\")\n",
    "\n",
    "        self.val_x = val_x\n",
    "        print(\"Read in Val X data\")\n",
    "\n",
    "        self.val_y = val_y\n",
    "        print(\"Read in Val y data\")\n",
    "\n",
    "        self.test_x = test_x\n",
    "        print(\"Read in Test X data\")\n",
    "\n",
    "        self.test_y = test_y\n",
    "        print(\"Read in Test y data\")\n",
    "\n",
    "\n",
    "\n",
    "    def read_in_model(self, model, type):\n",
    "        \"\"\" Reads in underlying model object for tuning, and also read in what type of model it is \"\"\"\n",
    "\n",
    "        assert type == 'Classification' or type == 'Regression' # check\n",
    "\n",
    "        # record\n",
    "        self.model = model\n",
    "        self.clf_type = type \n",
    "\n",
    "        print(f'Successfully read in model {self.model}, which is a {self.clf_type} model')\n",
    "\n",
    "\n",
    "\n",
    "    def set_hyperparameters(self, parameter_choices):\n",
    "        \"\"\" Input hyperparameter choices \"\"\"\n",
    "\n",
    "        self.parameter_choices = parameter_choices\n",
    "        self._sort_hyperparameter_choices()\n",
    "\n",
    "        self.hyperparameters = list(parameter_choices.keys())\n",
    "\n",
    "        # automatically calculate how many different values in each hyperparameter\n",
    "        self.n_items = [len(parameter_choices[key]) for key in self.hyperparameters]\n",
    "\n",
    "        # automatically calculate all combinations and setup checked and result arrays and tuning result dataframe\n",
    "        self._get_combinations()\n",
    "        self._get_checked_and_result_array()\n",
    "        self._setup_tuning_result_df()\n",
    "\n",
    "        print(\"Successfully recorded hyperparameter choices\")\n",
    "\n",
    "\n",
    "\n",
    "    def _sort_hyperparameter_choices(self):\n",
    "        \"\"\" Helper to ensure all hyperparameter choice values are in order from lowest to highest \"\"\"\n",
    "\n",
    "        for key in self.parameter_choices:\n",
    "            tmp = copy.deepcopy(list(self.parameter_choices[key]))\n",
    "            tmp.sort()\n",
    "            self.parameter_choices[key] = tuple(tmp)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_combinations(self):\n",
    "        \"\"\" Helper to calculate all combinations \"\"\"\n",
    "\n",
    "        ##ALGORITHM\n",
    "\n",
    "        # recursively append values to get every combination in ordinal/numerical form\n",
    "        self.combos = [[]]\n",
    "        for i in range(len(self.n_items)):\n",
    "\n",
    "            tmp = copy.deepcopy(self.combos)\n",
    "            self.combos = list()\n",
    "\n",
    "            for x in tmp:\n",
    "\n",
    "                for k in range(self.n_items[i]):\n",
    "                    y = copy.deepcopy(x)\n",
    "                    \n",
    "                    y.append(k)\n",
    "\n",
    "                    self.combos.append(y)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_checked_and_result_array(self):\n",
    "        \"\"\" Helper to set up checked and result array \"\"\"\n",
    "\n",
    "        self.checked = np.zeros(shape=self.n_items)\n",
    "        self.result = np.zeros(shape=self.n_items)\n",
    "\n",
    "\n",
    "\n",
    "    def _setup_tuning_result_df(self):\n",
    "        \"\"\" Helper to set up tuning result dataframe \"\"\"\n",
    "\n",
    "        tune_result_columns = copy.deepcopy(self.hyperparameters)\n",
    "\n",
    "        # Different set of metric columns for different types of models\n",
    "        if self.clf_type == 'Classification':\n",
    "            tune_result_columns.extend(self.classification_extra_output_columns)\n",
    "        elif self.clf_type == 'Regression':\n",
    "            tune_result_columns.extend(self.regression_extra_output_columns)\n",
    "\n",
    "        self.tuning_result = pd.DataFrame({col:list() for col in tune_result_columns})\n",
    "\n",
    "\n",
    "    \n",
    "    def change_tuning_style(self, type, seed = None, outer_most_layer = 2, randomise = True): \n",
    "        # Function which determines how to order the combinations for tuning\n",
    "\n",
    "        if not self.combos:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "\n",
    "        self.combos.sort() # to ensure functionality of seed, always sort first\n",
    "\n",
    "        if type == 'a': # Sorted order (nested loops)\n",
    "            # sorting operation conducted previously\n",
    "            print('Changed tuning order to sorted')\n",
    "        \n",
    "        elif type == 'b': # Random order\n",
    "            \n",
    "            if seed:\n",
    "                random.seed(seed)\n",
    "            else:\n",
    "                random.seed(self._seed)\n",
    "            \n",
    "            random.shuffle(self.combos)\n",
    "            print('Changed tuning_order to randomised')\n",
    "        \n",
    "        elif type == 'c': # Layer by Layer \n",
    "            self._change_tuning_style_c(outer_most_layer, randomise, seed)\n",
    "            if randomise:\n",
    "                print(f'Changed tuning order to \"Layer by Layer\": {self.outmost_layer} Layers, randomised')\n",
    "            else:\n",
    "                print(f'Changed tuning order to \"Layer by Layer\": {self.outmost_layer} Layers, not randomised')\n",
    "\n",
    "        elif type == 'd': # Diagonal + Horizontal first, before conducting layer by layer\n",
    "            self._change_tuning_style_d()\n",
    "            print(f'Changed tuning order to Diag-Hor -> \"Layer by Layer\": {self.outmost_layer} Layers, randomised')\n",
    "\n",
    "\n",
    "\n",
    "    def _get_core(self):\n",
    "        \"\"\" Helper to calculate core \"\"\"\n",
    "        self._core = [int(i/2) for i in self.n_items]\n",
    "\n",
    "\n",
    "\n",
    "    def _get_relative_combos(self):\n",
    "        \"\"\" Helper to calculate relative coordinates of combinations\"\"\" \n",
    "        self._relative_combos = [[combo[j] - self._core[j] for j in range(len(self.n_items))] for combo in self.combos]\n",
    "\n",
    "\n",
    "\n",
    "    ### TYPE C\n",
    "    def _get_both_combos(self):\n",
    "        \"\"\" Helper to put (combos, relative combos) together into a tuple \"\"\"\n",
    "        self._get_relative_combos()\n",
    "        self._both_combos = [[self.combos[i], self._relative_combos[i]] for i in range(len(self.combos))]\n",
    "\n",
    "\n",
    "\n",
    "    def _get_layer_by_layer(self):\n",
    "        \"\"\" Helper to get Layer by Layer order \"\"\"\n",
    "\n",
    "        retain = copy.deepcopy(self._both_combos)\n",
    "        self._dealt_with = list()\n",
    "\n",
    "        # starting with the outmost layer, ending with -1 because of the >\n",
    "        for i in range(self.outmost_layer, -2, -1):\n",
    "            tmp_retain = list()\n",
    "            tmp_dealt_with = list()\n",
    "            \n",
    "            for item in retain:\n",
    "\n",
    "                trigger = 1\n",
    "                for j in item[1]:\n",
    "                    if abs(j) > i:\n",
    "                        tmp_dealt_with.append(item[0])\n",
    "                        trigger = 0\n",
    "                        break\n",
    "                \n",
    "                if trigger:\n",
    "                    tmp_retain.append(item)\n",
    "\n",
    "            retain = tmp_retain\n",
    "            self._dealt_with.append(tmp_dealt_with)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_c_combos(self):\n",
    "        \"\"\" Helper to get the combinations into Layer by Layer order \"\"\"\n",
    "\n",
    "        self.combos = list()\n",
    "        # working with dealt_with backwards because we want the innermost layers first\n",
    "        for i in range(len(self._dealt_with)-1, -1, -1):\n",
    "            self.combos.extend(self._dealt_with[i])\n",
    "            \n",
    "\n",
    "\n",
    "    def _change_tuning_style_c(self, outmost_layer, randomise, seed):\n",
    "        \"\"\" Helper to run all type-c helpers to get combinations into Layer by Layer order \"\"\"\n",
    "\n",
    "        if randomise:\n",
    "            if seed:\n",
    "                random.seed(seed)\n",
    "            else:\n",
    "                random.seed(self._seed)\n",
    "        \n",
    "        random.shuffle(self.combos)\n",
    "\n",
    "        self.outmost_layer = outmost_layer\n",
    "        self._get_core()\n",
    "        self._get_both_combos()\n",
    "        self._get_layer_by_layer()\n",
    "        self._get_c_combos()\n",
    "\n",
    "\n",
    "\n",
    "    ### TYPE D    \n",
    "    def _change_tuning_style_d(self):\n",
    "        \"\"\" Helper to all type-d helpers to get combinations into Layer by Layer order \"\"\"\n",
    "\n",
    "        self._change_tuning_style_c(outmost_layer= max(self.n_items), randomise = True, seed = self._seed)\n",
    "\n",
    "        self._get_hor_combos()\n",
    "        self._get_diag_combos()\n",
    "\n",
    "        self._get_d_combos()\n",
    "\n",
    "\n",
    "\n",
    "    def _get_hor_combos(self):\n",
    "        \"\"\" Helper to get all combinations that lie on horizontal line from core \"\"\"\n",
    "        self._hor_combos = list()\n",
    "        for i in range(len(self.n_items)):\n",
    "            for j in range(self.n_items[i]):\n",
    "                tmp = copy.deepcopy(self._core)\n",
    "                tmp[i] = j\n",
    "                self._hor_combos.append(tmp)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_pos_neg_combos(self):\n",
    "        \"\"\" Helper to get all combinations of -1 and 1 \"\"\"\n",
    "\n",
    "        ##ALGORITHM\n",
    "        self._pos_neg_combos = [[]]\n",
    "        for i in range(len(self.n_items)):\n",
    "\n",
    "            tmp = copy.deepcopy(self._pos_neg_combos)\n",
    "            self._pos_neg_combos = list()\n",
    "\n",
    "            for x in tmp:\n",
    "\n",
    "                for k in (-1, 1):\n",
    "                    y = copy.deepcopy(x)\n",
    "                    \n",
    "                    y.append(k)\n",
    "\n",
    "                    self._pos_neg_combos.append(y)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_abs_max(self):\n",
    "        \"\"\" Helper to get maximum absolute value of the relative combos \"\"\"\n",
    "\n",
    "        max_pos = max([self.n_items[i] - self._core[i] for i in range(len(self.n_items))])\n",
    "        min_neg = min([0-self._core[i] for i in range(len(self.n_items))])\n",
    "\n",
    "        self._abs_max = max((max_pos, abs(min_neg)))\n",
    "\n",
    "\n",
    "\n",
    "    def _get_diag_combos(self):\n",
    "        \"\"\" Helper to get all combinations that lie on diagonal line from core \"\"\"\n",
    "\n",
    "        # Implementation idea: first get all the diagonal combos (even those that go outside the field-space) \n",
    "        # by using all combinations of (-1, 1) multipled by each value from 1 to _abs_max, before eliminating combos that go\n",
    "        # outside the field-space\n",
    "        self._get_pos_neg_combos()\n",
    "        self._get_abs_max()\n",
    "\n",
    "        diag_rel_combos = list()\n",
    "        for i in range(self._abs_max):\n",
    "            diag_rel_combos.extend([[(i+1)*pos_neg_combo[j] for j in range(len(self.n_items))] for pos_neg_combo in self._pos_neg_combos])\n",
    "        \n",
    "        tmp_diag_combos = [[combo[j] + self._core[j] for j in range(len(self.n_items))] for combo in diag_rel_combos]\n",
    "\n",
    "        self._diag_combos = list()\n",
    "        \n",
    "        for combo in tmp_diag_combos:\n",
    "            trigger = 1\n",
    "            for i in range(len(combo)):\n",
    "                if combo[i] < 0 or combo[i] >= self.n_items[i]: # if outside field space then eliminate\n",
    "                    trigger = 0\n",
    "                    break\n",
    "            \n",
    "            if trigger:\n",
    "                self._diag_combos.append(combo)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_d_combos(self):\n",
    "        \"\"\" Helper to run all type-c helpers to get combinations into Diag-Hor -> Layer by Layer order \"\"\"\n",
    "\n",
    "        self._new_combos = list()\n",
    "\n",
    "        self._new_combos.append(self._core)\n",
    "\n",
    "        # put in diagonal first to get more variety\n",
    "        for combo in self._diag_combos:\n",
    "            if combo not in self._new_combos:\n",
    "                self._new_combos.append(combo)\n",
    "\n",
    "        for combo in self._hor_combos:\n",
    "            if combo not in self._new_combos:\n",
    "                self._new_combos.append(combo)\n",
    "\n",
    "        # put in rest of the combos - already sorted in layer by layer order\n",
    "        for combo in self.combos:\n",
    "            if combo not in self._new_combos:\n",
    "                self._new_combos.append(combo)\n",
    "        \n",
    "        self.combos = self._new_combos\n",
    "\n",
    "\n",
    "        \n",
    "    def tune(self): #TODO\n",
    "        \"\"\" Begin tuning \"\"\"\n",
    "\n",
    "        if self.train_x is None or self.train_y is None or self.val_x is None or self.val_y is None or self.test_x is None or self.test_y is None:\n",
    "            print(\" Missing one of the datasets, please run .read_in_data() \")\n",
    "            return\n",
    "\n",
    "        if self.model is None:\n",
    "            print(\" Missing model, please run .read_in_model() \")\n",
    "            return\n",
    "        \n",
    "        if self.combos is None:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "\n",
    "        if self.tuning_result_saving_address is None:\n",
    "            print(\"Missing tuning result csv saving address, please run ._save_tuning_result() first\")\n",
    "\n",
    "\n",
    "        self._up_to = 0     # reset\n",
    "\n",
    "        for combo in self.combos:\n",
    "            \n",
    "            self._up_to += 1\n",
    "\n",
    "            if not self.checked[tuple(combo)]:\n",
    "\n",
    "                self._train_and_test_combo(combo)\n",
    "            \n",
    "            else:\n",
    "                print(f'Already Trained and Tested combination {self._up_to}')\n",
    "      \n",
    "\n",
    "\n",
    "    def _train_and_test_combo(self, combo):\n",
    "        \"\"\" Helper to train and test each combination as part of tune() \"\"\"\n",
    "\n",
    "        combo = tuple(combo)\n",
    "        \n",
    "        params = {self.hyperparameters[i]:self.parameter_choices[self.hyperparameters[i]][combo[i]] for i in range(len(self.hyperparameters))}\n",
    "\n",
    "        # initialise object\n",
    "        clf = self.model(**params)\n",
    "\n",
    "        # get time and fit\n",
    "        start = time.time()\n",
    "        clf.fit(self.train_x, self.train_y)\n",
    "        end = time.time()\n",
    "\n",
    "        # get predicted labels/values for three datasets\n",
    "        train_pred = clf.predict(self.train_x)\n",
    "        val_pred = clf.predict(self.val_x)\n",
    "        test_pred = clf.predict(self.test_x)\n",
    "\n",
    "        # get scores and time used\n",
    "        time_used = end-start\n",
    "\n",
    "        # build output dictionary and save result\n",
    "        df_building_dict = params\n",
    "\n",
    "        if self.clf_type == 'Regression':\n",
    "            train_score = r2_score(self.train_y, train_pred)\n",
    "            val_score = r2_score(self.val_y, val_pred)\n",
    "            test_score = r2_score(self.test_y, test_pred)\n",
    "\n",
    "            train_rmse = np.sqrt(mean_squared_error(self.train_y, train_pred))\n",
    "            val_rmse = np.sqrt(mean_squared_error(self.val_y, val_pred))\n",
    "            test_rmse = np.sqrt(mean_squared_error(self.test_y, test_pred))\n",
    "\n",
    "            train_mape = mean_absolute_percentage_error(self.train_y, train_pred)\n",
    "            val_mape = mean_absolute_percentage_error(self.val_y, val_pred)\n",
    "            test_mape = mean_absolute_percentage_error(self.test_y, test_pred)\n",
    "\n",
    "            df_building_dict['Train r2'] = [np.round(train_score, 4)]\n",
    "            df_building_dict['Val r2'] = [np.round(val_score, 4)]\n",
    "            df_building_dict['Test r2'] = [np.round(test_score, 4)]\n",
    "            df_building_dict['Train RMSE'] = [np.round(train_rmse, 4)]\n",
    "            df_building_dict['Val RMSE'] = [np.round(val_rmse, 4)]\n",
    "            df_building_dict['Test RMSE'] = [np.round(test_rmse, 4)]\n",
    "            df_building_dict['Train MAPE'] = [np.round(train_mape, 4)]\n",
    "            df_building_dict['Val MAPE'] = [np.round(val_mape, 4)]\n",
    "            df_building_dict['Test MAPE'] = [np.round(test_mape, 4)]\n",
    "            df_building_dict['Time'] = [np.round(time_used, 2)]\n",
    "\n",
    "        \n",
    "        elif self.clf_type == 'Classification':\n",
    "            train_score = accuracy_score(self.train_y, train_pred)\n",
    "            val_score = clf.score(self.val_y, val_pred)\n",
    "            test_score = clf.score(self.test_y, test_pred)\n",
    "\n",
    "            train_bal_accu = balanced_accuracy_score(self.train_y, train_pred)\n",
    "            val_bal_accu = balanced_accuracy_score(self.val_y, val_pred)\n",
    "            test_bal_accu = balanced_accuracy_score(self.test_y, test_pred)\n",
    "\n",
    "            train_f1 = f1_score(self.train_y, train_pred, average='weighted')\n",
    "            val_f1 = f1_score(self.val_y, val_pred, average='weighted')\n",
    "            test_f1 = f1_score(self.test_y, test_pred, average='weighted')\n",
    "\n",
    "            train_precision = precision_score(self.train_y, train_pred, average='weighted')\n",
    "            val_precision = precision_score(self.val_y, val_pred, average='weighted')\n",
    "            test_precision = precision_score(self.test_y, test_pred, average='weighted')\n",
    "        \n",
    "            train_recall = recall_score(self.train_y, train_pred, average='weighted')\n",
    "            val_recall = recall_score(self.val_y, val_pred, average='weighted')\n",
    "            test_recall = recall_score(self.test_y, test_pred, average='weighted')\n",
    "\n",
    "            df_building_dict['Train accu'] = [np.round(train_score, 4)]\n",
    "            df_building_dict['Val accu'] = [np.round(val_score, 4)]\n",
    "            df_building_dict['Test accu'] = [np.round(test_score, 4)]\n",
    "            df_building_dict['Train balanced_accuracy'] = [np.round(train_bal_accu, 4)]\n",
    "            df_building_dict['Val balanced_accuracy'] = [np.round(val_bal_accu, 4)]\n",
    "            df_building_dict['Test balanced_accuracy'] = [np.round(test_bal_accu, 4)]\n",
    "            df_building_dict['Train f1'] = [np.round(train_f1, 4)]\n",
    "            df_building_dict['Val f1'] = [np.round(val_f1, 4)]\n",
    "            df_building_dict['Test f1'] = [np.round(test_f1, 4)]\n",
    "            df_building_dict['Train precision'] = [np.round(train_precision, 4)]\n",
    "            df_building_dict['Val precision'] = [np.round(val_precision, 4)]\n",
    "            df_building_dict['Test precision'] = [np.round(test_precision, 4)]\n",
    "            df_building_dict['Train recall'] = [np.round(train_recall, 4)]\n",
    "            df_building_dict['Val recall'] = [np.round(val_recall, 4)]\n",
    "            df_building_dict['Test recall'] = [np.round(test_recall, 4)]\n",
    "            df_building_dict['Time'] = [np.round(time_used, 2)]\n",
    "\n",
    "        tmp = pd.DataFrame(df_building_dict)\n",
    "\n",
    "        self.tuning_result = self.tuning_result.append(tmp)\n",
    "        self._save_tuning_result()\n",
    "\n",
    "        # update best score stats\n",
    "        if val_score > self.best_score: \n",
    "            self.best_score = val_score\n",
    "            self.best_clf = clf\n",
    "            self.best_combo = combo\n",
    "\n",
    "        # update internal governing DataFrames\n",
    "        self.checked[combo] = 1\n",
    "        self.result[combo] = val_score\n",
    "\n",
    "\n",
    "        print(f'''Trained and Tested combination {self._up_to}, taking {np.round(time_used, 2)} seconds\n",
    "        Current best combo: {self.best_combo} with val score {self.best_score}''')\n",
    "\n",
    "\n",
    "\n",
    "    def _save_tuning_result(self):\n",
    "        \"\"\" Helper to export tuning result csv \"\"\"\n",
    "\n",
    "        self.tuning_result.to_csv(f'{self.tuning_result_saving_address}.csv', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def view_best_combo_and_score(self):\n",
    "        \"\"\" View best combination and its validation score \"\"\"\n",
    "        \n",
    "        print(f'(Current) Best combo: {self.best_combo} with val score {self.best_score}')\n",
    "\n",
    "    \n",
    "\n",
    "    def read_in_tuning_result_df(self, address): \n",
    "        \"\"\" Read in tuning result csv and read data into checked and result arrays \"\"\"\n",
    "\n",
    "        if self.parameter_choices is None:\n",
    "            print(\"Missing parameter_choices to build parameter_value_map_index, please run set_hyperparameters() first\")\n",
    "\n",
    "        if self.clf_type is None:\n",
    "            print('Missing clf_type. Please run .read_in_model() first.')\n",
    "\n",
    "        self.tuning_result = pd.read_csv(address)\n",
    "        print(f\"Successfully read in tuning result of {len(self.tuning_result)} rows\")\n",
    "\n",
    "        self._create_parameter_value_map_index()\n",
    "\n",
    "        # read DataFrame data into internal governing DataFrames of JiXi\n",
    "        for row in self.tuning_result.iterrows():\n",
    "    \n",
    "            combo = tuple([self._parameter_value_map_index[hyperparam][row[1][hyperparam]] for hyperparam in self.hyperparameters])\n",
    "            \n",
    "            self.checked[combo] = 1\n",
    "            \n",
    "            if self.clf_type == 'Regression':\n",
    "                self.result[combo] = row[1]['Val r2']\n",
    "            elif self.clf_type == 'Classification':\n",
    "                self.result[combo] = row[1]['Val accu']\n",
    "        \n",
    "            # update best score stats\n",
    "            if self.result[combo] > self.best_score: \n",
    "                self.best_score = self.result[combo]\n",
    "                self.best_clf = None\n",
    "                print(f\"As new Best Combo {combo} is read in, best_clf is set to None\")\n",
    "                self.best_combo = combo\n",
    "\n",
    "\n",
    "    \n",
    "    def _create_parameter_value_map_index(self):\n",
    "        \"\"\" Helper to create parameter-value index map \"\"\"\n",
    "\n",
    "        self._parameter_value_map_index = dict()\n",
    "        for key in self.parameter_choices.keys():\n",
    "            tmp = dict()\n",
    "            for i in range(len(self.parameter_choices[key])):\n",
    "                tmp[self.parameter_choices[key][i]] = i\n",
    "            self._parameter_value_map_index[key] = tmp\n",
    "    \n",
    "\n",
    "\n",
    "    def set_tuning_result_saving_address(self, address):\n",
    "        \"\"\" Read in where to save tuning object \"\"\"\n",
    "\n",
    "        self.tuning_result_saving_address = address\n",
    "        print('Successfully set tuning output address')\n",
    "\n",
    "\n",
    "    \n",
    "    def _set_object_saving_address(self, address):\n",
    "        \"\"\" Read in where to save the JiXi object \"\"\"\n",
    "\n",
    "        self.object_saving_address = address\n",
    "        print('Successfully set object output address')\n",
    "\n",
    "\n",
    "\n",
    "    def export_jixi(self, address):\n",
    "        \"\"\" Export jixi object \"\"\"\n",
    "\n",
    "        self._set_object_saving_address(address)\n",
    "\n",
    "        # copy object and set big objects to None\n",
    "        object_save = copy.deepcopy(self)\n",
    "        \n",
    "        object_save.train_x = None\n",
    "        object_save.train_y = None\n",
    "        object_save.val_x = None\n",
    "        object_save.val_y = None\n",
    "        object_save.test_x = None\n",
    "        object_save.test_y = None\n",
    "        object_save._up_to = 0\n",
    "\n",
    "        # Export\n",
    "        with open(f'{self.object_saving_address}.pickle', 'wb') as f:\n",
    "            pickle.dump(object_save, f)\n",
    "\n",
    "        print(f'Successfully exported JiXi object as {self.object_saving_address}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ZhongShan import *\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NingXiang:\n",
    "\n",
    "    def __init__(self, sanmin):\n",
    "        \"\"\" Initialise class \"\"\"\n",
    "        self.sanmin = sanmin\n",
    "        self._initialise_objects()\n",
    "\n",
    "        print('NingXiang Initialised')\n",
    "\n",
    "\n",
    "\n",
    "    def _initialise_objects(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_feature_combinations(self, score_type, label, penalty_function_type, export_address = None):\n",
    "        \"\"\" Function that gets combinations based on NingXiang's algorithm along with its score, based on\n",
    "        inputted score type, label and penalty function type. Has option to export\"\"\"\n",
    "\n",
    "        if score_type not in ('NMI', 'Abs Corr'):\n",
    "            print(\"score_type should be either 'NMI' or 'Abs Corr'\")\n",
    "            return\n",
    "        \n",
    "        if label not in self.sanmin.label_columns:\n",
    "            print(\"label should be in the designated labels column\")\n",
    "            return\n",
    "\n",
    "        if penalty_function_type not in ('None', 'Mean', 'Max'):\n",
    "            print(\"penalty_function_type should be either 'None' or 'Mean' or 'Max'\")\n",
    "            return\n",
    "\n",
    "        # get the correct matrix\n",
    "        if score_type == 'Abs Corr':\n",
    "            score_matrix = self.sanmin.abs_corr_matrix\n",
    "        elif score_type == 'NMI':\n",
    "            score_matrix = self.sanmin.NMI_matrix\n",
    "            \n",
    "        # get the correct penalty function\n",
    "        if penalty_function_type == 'None':\n",
    "            funct = self._return_zero\n",
    "        elif penalty_function_type == 'Mean':\n",
    "            funct = np.mean\n",
    "        elif penalty_function_type == 'Max':\n",
    "            funct = max\n",
    "\n",
    "        # object to output\n",
    "        feature_combos_with_score = list()\n",
    "\n",
    "        # starting with each individual feature\n",
    "        for feature in self.sanmin.final_features[label]:\n",
    "            if feature in self.sanmin.label_columns: # don't add if it is a label\n",
    "                continue\n",
    "\n",
    "            # initial current combo\n",
    "            curr_combo = [feature]\n",
    "\n",
    "            # initial current score\n",
    "            curr_combo_score = score_matrix.loc[feature][label]\n",
    "\n",
    "            # initial combo appended with its score\n",
    "            feature_combos_with_score.append((copy.deepcopy(curr_combo), curr_combo_score))\n",
    "\n",
    "            switch = True\n",
    "\n",
    "            while switch is True:\n",
    "\n",
    "                # temporary scores\n",
    "                curr_added_value = 0\n",
    "                curr_feature_to_add = None\n",
    "\n",
    "                # for all try-able combinations\n",
    "                for candidate_feature in self.sanmin.final_features[label]: \n",
    "                    if candidate_feature in curr_combo or candidate_feature in self.sanmin.label_columns: # don't try those already in, nor those that are labels\n",
    "                        continue\n",
    "                    \n",
    "                    # get candidate's own corr with label\n",
    "                    candidate_feature_score = score_matrix.loc[candidate_feature][label]\n",
    "\n",
    "                    # get list of corr between candidate and features currently in combo\n",
    "                    candidate_feature_relation_scores = list()\n",
    "                    for curr_combo_feature in curr_combo:\n",
    "                        candidate_feature_relation_scores.append(score_matrix.loc[candidate_feature][curr_combo_feature])\n",
    "                    \n",
    "                    # if candidate score - penalty > current best, then accept; else, don't accept\n",
    "                    if candidate_feature_score - funct(candidate_feature_relation_scores) >= curr_added_value:\n",
    "                        curr_added_value = candidate_feature_score - funct(candidate_feature_relation_scores)\n",
    "                        curr_feature_to_add = candidate_feature\n",
    "\n",
    "                # if managed to add something to combo, then continue loop and add to overall list, else break this loop\n",
    "                if curr_feature_to_add is None:\n",
    "                    switch = False\n",
    "\n",
    "                else:\n",
    "                    curr_combo.append(curr_feature_to_add)\n",
    "                    curr_combo_score += curr_added_value\n",
    "                    feature_combos_with_score.append((copy.deepcopy(curr_combo), curr_combo_score))\n",
    "\n",
    "        # Remove duplicates and sort\n",
    "        feature_combos_with_score = self._features_duplicate_removal(feature_combos_with_score)\n",
    "        feature_combos_with_score.sort(key = lambda x: x[1])\n",
    "\n",
    "        # Get pure feature combinations, and scores of feature combinations\n",
    "        feature_combo = [feature_combo[0] for feature_combo in feature_combos_with_score]\n",
    "        feature_combo_scores = [feature_combo[1] for feature_combo in feature_combos_with_score]\n",
    "\n",
    "        print(f\"{len(feature_combos_with_score)} feature combinations, with combo scores ranging from {round(feature_combo_scores[0], 4)} to {round(feature_combo_scores[-1], 4)}\")\n",
    "        \n",
    "\n",
    "        # Export combinations and scores as a json\n",
    "        if export_address:\n",
    "            json_output = {'feature_combos_with_score': feature_combos_with_score, \n",
    "                            'feature_combo': feature_combo, \n",
    "                            'feature_combo_scores': feature_combo_scores}\n",
    "                            \n",
    "            with open(f'{export_address}.json', 'w') as f:\n",
    "                json.dump(json_output, f, indent=2) \n",
    "            print(\"Export Completed\")\n",
    "    \n",
    "        return feature_combos_with_score, feature_combo, feature_combo_scores\n",
    "\n",
    "\n",
    "    def _return_zero(self, dummy):\n",
    "        \"\"\" Helper function that returns 0 for penalty, no matter input \"\"\"\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "    def _features_duplicate_removal(self, feature_combos_with_score):\n",
    "        \"\"\" Helper function that remove duplicate combinations \"\"\"\n",
    "        for i in range(len(feature_combos_with_score)):\n",
    "            feature_combos_with_score[i][0].sort()\n",
    "\n",
    "        feature_combos_with_score.sort(key = lambda x:x[0])\n",
    "\n",
    "        duplicate_i = []\n",
    "\n",
    "        feature_combos_no_duplicates = []\n",
    "\n",
    "        for i in range(len(feature_combos_with_score)-1):\n",
    "            if i in duplicate_i:\n",
    "                continue\n",
    "\n",
    "            if feature_combos_with_score[i][0] == feature_combos_with_score[i+1][0]:\n",
    "                # retain the higher score\n",
    "                if feature_combos_with_score[i][1] >= feature_combos_with_score[i+1][1]:\n",
    "                    duplicate_i.append(i)\n",
    "                else:\n",
    "                    duplicate_i.append(i+1)\n",
    "            \n",
    "            if i not in duplicate_i:\n",
    "                feature_combos_no_duplicates.append(feature_combos_with_score[i])\n",
    "\n",
    "        return feature_combos_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from itertools import combinations \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SanMin:\n",
    "\n",
    "    def __init__(self, input, input_type):\n",
    "\n",
    "        \n",
    "        if input_type not in ('ZhongShan', 'Components'):\n",
    "            print('input_type must be either \"ZhongShan\" or \"Components\"')\n",
    "            return \n",
    "\n",
    "        if input_type == 'ZhongShan':\n",
    "            self.OHE_storage = copy.deepcopy(input.OHE_storage)\n",
    "            self.pca = copy.deepcopy(input.pca)\n",
    "            self.final_ncomponents = copy.deepcopy(input.final_ncomponents)\n",
    "            self.standardiser_objects = copy.deepcopy(input.standardiser_objects)\n",
    "            self.final_features = copy.deepcopy(input.final_features)\n",
    "            self.retained_columns = copy.deepcopy(input.retained_columns)\n",
    "            self.label_columns = copy.deepcopy(input.label_columns)\n",
    "            self.abs_corr_matrix = copy.deepcopy(input.abs_corr_matrix)\n",
    "            self.NMI_matrix = copy.deepcopy(input.NMI_matrix)\n",
    "            \n",
    "        elif input_type == 'Components':\n",
    "\n",
    "            with open(f'{input}', 'rb') as f:\n",
    "                sanmin_components = pickle.load(f)\n",
    "            \n",
    "            self.OHE_storage = sanmin_components['OHE_storage']\n",
    "            self.pca = sanmin_components['pca']\n",
    "            self.final_ncomponents = sanmin_components['final_ncomponents']\n",
    "            self.standardiser_objects = sanmin_components['standardiser_objects']\n",
    "            self.final_features = sanmin_components['final_features']\n",
    "            self.retained_columns = sanmin_components['retained_columns']\n",
    "            self.label_columns = sanmin_components['label_columns']\n",
    "            self.abs_corr_matrix = sanmin_components['abs_corr_matrix']\n",
    "            self.NMI_matrix = sanmin_components['NMI_matrix']\n",
    "        \n",
    "\n",
    "        self.feature_selected_future_data = None\n",
    "\n",
    "\n",
    "\n",
    "    def import_future_data(self, future_data, toggle_index = True):\n",
    "        \"\"\" Read in Future Data for transformation \"\"\"\n",
    "\n",
    "        self.future_data = future_data\n",
    "\n",
    "        if toggle_index:\n",
    "            self.future_data = self.reset_index(self.future_data)\n",
    "            print('Reset index successful')\n",
    "        else:\n",
    "            print('Did not reset index upon request')\n",
    "\n",
    "\n",
    "\n",
    "    def export_data(self, label, address, index=False):\n",
    "        \"\"\" export the manipulated future data \"\"\"\n",
    "\n",
    "        if self.feature_selected_future_data is None:\n",
    "            print('Please run .get_feature_selected_data() before re-attempting')\n",
    "            return\n",
    "        \n",
    "        self.feature_selected_future_data[label].to_csv(f'{address}.csv', index=index)\n",
    "\n",
    "\n",
    "\n",
    "    def standardise_transform(self):\n",
    "        \"\"\" Transform data using pre-fitted standardiser \"\"\"\n",
    "\n",
    "        if self.standardiser_objects is None:\n",
    "            print(\"No standardiser in this SanMin\")\n",
    "            return\n",
    "\n",
    "        if self.future_data is None:\n",
    "            print(\"Please input Future Data before using this function\")\n",
    "            return\n",
    "        else:\n",
    "            for col in self.retained_columns:\n",
    "                standardiser = self.standardiser_objects[col]\n",
    "                standardiser_output = standardiser.transform(self.future_data[[col]])\n",
    "                standardiser_list_output = [x[0] for x in standardiser_output]\n",
    "\n",
    "                self.future_data[col] = standardiser_list_output\n",
    "\n",
    "        print(f'Standardised all retained columns in Future Data')\n",
    "    \n",
    "\n",
    "\n",
    "    def fill_na(self, fill_value=0): \n",
    "        \"\"\" Helper to fill na values in data with default value 0 \"\"\"\n",
    "\n",
    "        if self.future_data is None:\n",
    "            print(\"Please input Future Data before using this function\")\n",
    "            return\n",
    "        else:\n",
    "            self.future_data = self.future_data.fillna(fill_value)\n",
    "        \n",
    "        print(f'Filled null values on Future Data dataset with {fill_value}')\n",
    "\n",
    "\n",
    "\n",
    "    def one_hot_encode_transform(self, col_to_ohe): \n",
    "        \"\"\" OHE transform one column of data using pre-trained OHE object \"\"\"\n",
    "\n",
    "        if self.OHE_storage is None:\n",
    "            print(\"No OHE in this SanMin\")\n",
    "            return\n",
    "        \n",
    "        if col_to_ohe not in self.OHE_storage:\n",
    "            print(f\"No OHE of this column in this SanMin\")\n",
    "\n",
    "        if self.future_data is None:\n",
    "            print(\"Please input Full Data before using this function\")\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            OHE_output = pd.DataFrame.sparse.from_spmatrix(\\\n",
    "                self.OHE_storage[col_to_ohe]['OHE_object'].transform(self.future_data[[col_to_ohe]]))\n",
    "\n",
    "            for i in range(len(self.OHE_storage[col_to_ohe]['output_col_names'])):\n",
    "                self.future_data[self.OHE_storage[col_to_ohe]['output_col_names'][i]] = list(OHE_output[i])\n",
    "            \n",
    "            self.future_data = self.future_data.drop([col_to_ohe], axis = 1)\n",
    "        \n",
    "        print(f\"OHE'ed and Dropped '{col_to_ohe}' column on Future Data\")\n",
    "\n",
    "\n",
    "\n",
    "    def pca_transform(self): \n",
    "        \"\"\" PCA transform data using pre-trained PCA object \"\"\"\n",
    "\n",
    "        if self.pca is None:\n",
    "            print(\"No PCA object in this SanMin\")\n",
    "            return\n",
    "\n",
    "        if self.future_data is None:\n",
    "            print(\"Please input Future Data before using this function\")\n",
    "            return\n",
    "        else:\n",
    "            pca_tmp = self.pca.transform(self.future_data[self.feature_columns])\n",
    "        \n",
    "            pca_output = pd.DataFrame(pca_tmp)\n",
    "            pca_output.columns = [f'PCA {i}' for i in range(self.pca_n_components)]\n",
    "\n",
    "            for i in range(self.final_ncomponents):\n",
    "                self.future_data[f'PCA {i}'] = pca_output[f'PCA {i}']\n",
    "\n",
    "        print(f'PCA transformed Future Data, PLEASE REMEMBER TO USE .pca_update_features() to update feature_columns')\n",
    "            \n",
    "\n",
    "\n",
    "    def get_feature_selected_data(self):\n",
    "        \"\"\" Apply the pre-selected columns to Future Data data \"\"\"\n",
    "\n",
    "        if self.future_data is None:\n",
    "            print(\"Please input Future Data\")\n",
    "            return\n",
    "\n",
    "        if self.feature_columns is None:\n",
    "            print(\"Please run .set_columns() before re-attempting\")\n",
    "            return\n",
    "\n",
    "        self.feature_selected_future_data = dict()\n",
    "\n",
    "        for label in self.feature_columns:\n",
    "            feature_columns = self.final_features[label]\n",
    "            feature_columns.append(label)\n",
    "            \n",
    "            self.feature_selected_future_data[label] = self.future_data[feature_columns]\n",
    "\n",
    "    \n",
    "\n",
    "    def view_abs_corr_matrix(self):\n",
    "        \"\"\" View the absolute correlation matrix \"\"\"\n",
    "\n",
    "        if self.abs_corr_matrix is None:\n",
    "            print('The ZhongShan object which produced this SanMin did not have run .get_abs_corr()')\n",
    "            return\n",
    "\n",
    "        display(self.abs_corr_matrix)\n",
    "\n",
    "\n",
    "\n",
    "    def view_nmi_matrix(self):\n",
    "        \"\"\" View the NMI matrix \"\"\"\n",
    "\n",
    "        if self.NMI_matrix is None:\n",
    "            print('The ZhongShan object which produced this SanMin did not have run .get_nmi()')\n",
    "            print('Please run .get_nmi() before re-attempting')\n",
    "            return\n",
    "\n",
    "        display(self.NMI_matrix)\n",
    "\n",
    "\n",
    "\n",
    "    def export_SanMin(self, address):\n",
    "        with open(f'{address}.pickle', 'wb') as f:\n",
    "            pickle.dump(self, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_duplicate_removal(feature_combos_with_score):\n",
    "        \"\"\" Helper function that remove duplicate combinations \"\"\"\n",
    "        for i in range(len(feature_combos_with_score)):\n",
    "            feature_combos_with_score[i][0].sort()\n",
    "\n",
    "        feature_combos_with_score.sort(key = lambda x:x[0])\n",
    "\n",
    "        duplicate_i = []\n",
    "\n",
    "        feature_combos_no_duplicates = []\n",
    "\n",
    "        for i in range(len(feature_combos_with_score)-1):\n",
    "            if i in duplicate_i:\n",
    "                continue\n",
    "\n",
    "            if feature_combos_with_score[i][0] == feature_combos_with_score[i+1][0]:\n",
    "                duplicate_i.append(i)\n",
    "            \n",
    "            if i not in duplicate_i:\n",
    "                feature_combos_no_duplicates.append(feature_combos_with_score[i])\n",
    "\n",
    "        return feature_combos_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_df(model, x, y, combo):\n",
    "    pred_y = list((model.predict(x[combo])))\n",
    "\n",
    "    accu_df = pd.DataFrame({'pred': pred_y, 'y': list(y)})\n",
    "\n",
    "    accu_df['y_mean'] = np.mean(y)\n",
    "    \n",
    "    accu_df['TotErr'] = accu_df['y'] - accu_df['y_mean']\n",
    "    accu_df['TotSqErr'] = np.power(accu_df['TotErr'], 2)\n",
    "\n",
    "    accu_df['ResErr'] = accu_df['y'] - accu_df['pred']\n",
    "    accu_df['ResSqErr'] = np.power(accu_df['ResErr'], 2)\n",
    "\n",
    "    return accu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2(accu_df):\n",
    "\n",
    "    return 1 - sum(accu_df['ResSqErr'])/sum(accu_df['TotSqErr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_r2(accu_df, n, k):\n",
    "    \n",
    "    return 1-(sum(accu_df['ResSqErr'])/sum(accu_df['TotSqErr']))*((n-1)/(n-k-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_link(y_hat):\n",
    "    \n",
    "    p = 1/(1+np.exp(-y_hat))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def cloglog_link(y_hat):\n",
    "\n",
    "    p = 1-np.exp(-np.exp(y_hat))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def probit_link(y_hat):\n",
    "    \n",
    "    p = norm.cdf(y_hat)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "LINK_FUNCTION = {'logit': logit_link, 'cloglog': cloglog_link, 'probit': probit_link}\n",
    "LINK_INDEX = {'logit': 0, 'cloglog': 4, 'probit': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_restrict_llh(accu_df, link, n):\n",
    "\n",
    "    llh = n*np.log(1)\n",
    "    \n",
    "    for row in accu_df.iterrows():\n",
    "        p = link(row[1][0])\n",
    "\n",
    "        llh += row[1][1]*np.log(p/(1-p)) + np.log(1-p)\n",
    "\n",
    "\n",
    "    return llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_satur_llh(accu_df, link, n):\n",
    "\n",
    "    llh = n*np.log(1)\n",
    "    \n",
    "    for row in accu_df.iterrows():\n",
    "        p = link(row[1][1])\n",
    "\n",
    "        llh += row[1][1]*np.log(p/(1-p)) + np.log(1-p)\n",
    "\n",
    "    return llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aic(llh, k):\n",
    "\n",
    "    aic = 2*k - 2*llh\n",
    "    \n",
    "    return aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bic(llh, n, k):\n",
    "\n",
    "    bic = (k+1)*np.log(n) - 2*llh\n",
    "    \n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JiaoCheng Initialised\n",
      "1353 feature combinations, with combo scores ranging from 0.1145 to 11.8371\n",
      "416 feature combinations, with combo scores ranging from 0.1145 to 0.9953\n",
      "186 feature combinations, with combo scores ranging from 0.1145 to 0.7053\n",
      "N 1 logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/statsmodels/genmod/generalized_linear_model.py:1323: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_73328/3351300171.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mtest_llh_restrict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bin_restrict_llh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accu_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINK_FUNCTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLINK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mtrain_llh_satur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bin_satur_llh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accu_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINK_FUNCTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLINK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0mval_llh_satur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bin_satur_llh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accu_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINK_FUNCTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLINK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mtest_llh_satur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bin_satur_llh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accu_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINK_FUNCTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLINK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_73328/2687803095.py\u001b[0m in \u001b[0;36mget_bin_satur_llh\u001b[0;34m(accu_df, link, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mllh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccu_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5507\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5508\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5509\u001b[0;31m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5511\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mvalidate_all_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{type(self).__name__}.name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mvalidate_all_hashable\u001b[0;34m(error_name, *args)\u001b[0m\n\u001b[1;32m   1735\u001b[0m     \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m     \"\"\"\n\u001b[0;32m-> 1737\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{error_name} must be a hashable type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for DATA_TYPE in ('N', 'RS'):\n",
    "\n",
    "    sanmin = SanMin(f'../models/AFL_pipeline_{DATA_TYPE}_components.pickle', 'Components')\n",
    "\n",
    "    ningxiang = NingXiang(sanmin)\n",
    "\n",
    "\n",
    "    for LABEL in ('1', '2', '3'):\n",
    "\n",
    "        feature_combos_with_score_none, feature_combo_none, feature_combo_scores_none = ningxiang.get_feature_combinations('Abs Corr', LABEL, 'None')\n",
    "        feature_combos_with_score_mean, feature_combo_mean, feature_combo_scores_mean = ningxiang.get_feature_combinations('Abs Corr', LABEL, 'Mean')\n",
    "        feature_combos_with_score_max, feature_combo_max, feature_combo_scores_max = ningxiang.get_feature_combinations('Abs Corr', LABEL, 'Max')\n",
    "\n",
    "        all_combos = list()\n",
    "        all_combos.extend(feature_combos_with_score_none)\n",
    "        all_combos.extend(feature_combos_with_score_mean)\n",
    "        all_combos.extend(feature_combos_with_score_max)\n",
    "\n",
    "        all_combos = features_duplicate_removal(all_combos)\n",
    "\n",
    "\n",
    "        train_data = pd.read_csv(f'../data/curated/modelling/{DATA_TYPE}_Train_{LABEL}.csv')\n",
    "        val_data = pd.read_csv(f'../data/curated/modelling/{DATA_TYPE}_Validate_{LABEL}.csv')\n",
    "        test_data = pd.read_csv(f'../data/curated/modelling/{DATA_TYPE}_Test_{LABEL}.csv')\n",
    "\n",
    "        train_x = train_data.drop([LABEL], axis=1)\n",
    "        train_y = train_data[LABEL]\n",
    "        val_x = val_data.drop([LABEL], axis=1)\n",
    "        val_y = val_data[LABEL]\n",
    "        test_x = test_data.drop([LABEL], axis=1)\n",
    "        test_y = test_data[LABEL]\n",
    "\n",
    "\n",
    "        for LINK in ('logit', 'probit', 'cloglog'):\n",
    "\n",
    "\n",
    "            print(DATA_TYPE, LABEL, LINK)\n",
    "\n",
    "\n",
    "            tuning_result = pd.DataFrame()\n",
    "\n",
    "            for alpha in (0.01, 0.001, 0.0001, 0.00001):\n",
    "\n",
    "                for combo in all_combos:\n",
    "\n",
    "                    GLM_binom_fit = sm.GLM(train_y, train_x[combo[0]], family = families.Binomial(link=families.family.Binomial.links[LINK_INDEX[LINK]]())).fit_regularized(method='elastic_net', alpha=alpha, L1_wt=1.0, refit=True)\n",
    "\n",
    "                    train_accu_df = get_accuracy_df(GLM_binom_fit, train_x, train_y, combo[0])\n",
    "                    val_accu_df = get_accuracy_df(GLM_binom_fit, val_x, val_y, combo[0])\n",
    "                    test_accu_df = get_accuracy_df(GLM_binom_fit, test_x, test_y, combo[0])\n",
    "                    \n",
    "                    n_train = len(train_accu_df)\n",
    "                    n_val = len(val_accu_df)\n",
    "                    n_test = len(test_accu_df)\n",
    "                    k = len(combo)\n",
    "\n",
    "                    train_llh_restrict = get_bin_restrict_llh(train_accu_df, LINK_FUNCTION[LINK], n_train)\n",
    "                    val_llh_restrict = get_bin_restrict_llh(val_accu_df, LINK_FUNCTION[LINK], n_val)\n",
    "                    test_llh_restrict = get_bin_restrict_llh(test_accu_df, LINK_FUNCTION[LINK], n_test)\n",
    "\n",
    "                    train_llh_satur = get_bin_satur_llh(train_accu_df, LINK_FUNCTION[LINK], n_train)\n",
    "                    val_llh_satur = get_bin_satur_llh(val_accu_df, LINK_FUNCTION[LINK], n_val)\n",
    "                    test_llh_satur = get_bin_satur_llh(test_accu_df, LINK_FUNCTION[LINK], n_test)\n",
    "\n",
    "                    train_deviation = -2 * (train_llh_restrict - train_llh_satur)\n",
    "                    val_deviation = -2 * (val_llh_restrict - val_llh_satur)\n",
    "                    test_deviation = -2 * (test_llh_restrict - test_llh_satur)\n",
    "\n",
    "                    train_aic = get_aic(train_llh_restrict, k)\n",
    "                    val_aic = get_aic(val_llh_restrict, k)\n",
    "                    test_aic = get_aic(test_llh_restrict, k)\n",
    "\n",
    "                    train_bic = get_bic(train_llh_restrict, n_train, k)\n",
    "                    val_bic = get_bic(val_llh_restrict, n_val, k)\n",
    "                    test_bic = get_bic(test_llh_restrict, n_test, k)\n",
    "\n",
    "\n",
    "                    tmp = pd.DataFrame()\n",
    "                    tmp['combo'] = [combo[0]]\n",
    "                    tmp['score'] = [combo[1]]\n",
    "                    tmp['alpha'] = [alpha]\n",
    "\n",
    "                    tmp['Train Deviation'] = [train_deviation]\n",
    "                    tmp['Val Deviation'] = [val_deviation]\n",
    "                    tmp['Test Deviation'] = [test_deviation]\n",
    "\n",
    "                    # tmp['Train AIC'] = [GLM_binom_fit.aic]\n",
    "                    tmp['Train AIC'] = [train_aic]\n",
    "                    tmp['Val AIC'] = [val_aic]\n",
    "                    tmp['Test AIC'] = [test_aic]\n",
    "\n",
    "                    # tmp['Train BIC'] = [GLM_binom_fit.bic]\n",
    "                    tmp['Val BIC'] = [train_bic]\n",
    "                    tmp['Val BIC'] = [val_bic]\n",
    "                    tmp['Test BIC'] = [test_bic]\n",
    "\n",
    "                    # tmp['Train llh'] = [GLM_binom_fit.llf]\n",
    "                    tmp['Train llh'] = [train_llh_restrict]\n",
    "                    tmp['Val llh'] = [val_llh_restrict]\n",
    "                    tmp['Test llh'] = [test_llh_restrict]    \n",
    "\n",
    "                    tuning_result = tuning_result.append(tmp)\n",
    "\n",
    "                    tuning_result.to_csv(f'../models/tuning/{DATA_TYPE}_glm_{LINK}_{LABEL}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
