{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DXP.PNG\" style=\"width:350px;height:275px;\">\n",
    "<h2 style=\"color:red\">长 江 黄 河 永 不 倒 流 、改 开 之 路 绝 不 向 西 ！ 敬 爱 的 鄧 小 平 同 志 永 远 活 在 我 们 心 中</h2>\n",
    "<h5 style=\"color:red\"> - 二零二三年二月十九日</h5>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广安级（第三代）机器学习调参算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17/2/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuangAn:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialise class \"\"\"\n",
    "        self._initialise_objects()\n",
    "\n",
    "        print('GuangAn Initialised')\n",
    "\n",
    "\n",
    "\n",
    "    def _initialise_objects(self):\n",
    "        \"\"\" Helper to initialise objects \"\"\"\n",
    "\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.val_x = None\n",
    "        self.val_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        # self.tuning_result = None\n",
    "        self.model = None\n",
    "        # self.parameter_choices = None\n",
    "        # self.hyperparameters = None\n",
    "        # self.checked = None\n",
    "        # self.result = None\n",
    "        # self.tuning_result_saving_address = None\n",
    "        # self.object_saving_address = None\n",
    "        self._up_to = 0\n",
    "        # self._seed = 19421221\n",
    "        # self.best_score = -np.inf\n",
    "        # self.best_combo = None\n",
    "        # self.best_clf = None\n",
    "        self.clf_type = None\n",
    "\n",
    "\n",
    "        self.regression_extra_output_columns = ['Train r2', 'Val r2', 'Test r2', \n",
    "            'Train RMSE', 'Val RMSE', 'Test RMSE', 'Train MAPE', 'Val MAPE', 'Test MAPE', 'Time']\n",
    "        self.classification_extra_output_columns = ['Train accu', 'Val accu', 'Test accu', \n",
    "            'Train balanced_accu', 'Val balanced_accu', 'Test balanced_accu', 'Train f1', 'Val f1', 'Test f1', \n",
    "            'Train precision', 'Val precision', 'Test precision', 'Train recall', 'Val recall', 'Test recall', 'Time']\n",
    "\n",
    "        \n",
    "\n",
    "    def read_in_data(self, train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "        \"\"\" Reads in train validate test data for tuning \"\"\"\n",
    "\n",
    "        self.train_x = train_x\n",
    "        print(\"Read in Train X data\")\n",
    "\n",
    "        self.train_y = train_y\n",
    "        print(\"Read in Train x data\")\n",
    "\n",
    "        self.val_x = val_x\n",
    "        print(\"Read in Val X data\")\n",
    "\n",
    "        self.val_y = val_y\n",
    "        print(\"Read in Val y data\")\n",
    "\n",
    "        self.test_x = test_x\n",
    "        print(\"Read in Test X data\")\n",
    "\n",
    "        self.test_y = test_y\n",
    "        print(\"Read in Test y data\")\n",
    "\n",
    "\n",
    "\n",
    "    def read_in_model(self, model, type):\n",
    "        \"\"\" Reads in underlying model object for tuning, and also read in what type of model it is \"\"\"\n",
    "\n",
    "        assert type == 'Classification' or type == 'Regression' # check\n",
    "\n",
    "        # record\n",
    "        self.model = model\n",
    "        self.clf_type = type \n",
    "\n",
    "        print(f'Successfully read in model {self.model}, which is a {self.clf_type} model')\n",
    "\n",
    "\n",
    "\n",
    "    def set_hyperparameters(self, parameter_ranges_orig):\n",
    "        \"\"\" Input hyperparameter choices \"\"\"\n",
    "\n",
    "        self.parameter_ranges = parameter_ranges_orig\n",
    "        self._sort_hyperparameter_ranges()\n",
    "        \n",
    "\n",
    "        self.hyperparameters = list(self.parameter_ranges.keys())\n",
    "\n",
    "        self.categorical = {hp:False for hp in self.hyperparameters}\n",
    "        self.transform = {hp:False for hp in self.hyperparameters}\n",
    "\n",
    "        self._get_checked_dict()\n",
    "        self._setup_tuning_result_df()\n",
    "\n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        print(\"Successfully recorded hyperparameter choices\")\n",
    "\n",
    "\n",
    "\n",
    "    def _sort_hyperparameter_ranges(self):\n",
    "        \"\"\" Helper to ensure all hyperparameter choice values are in order from lowest to highest \"\"\"\n",
    "\n",
    "        for key in self.parameter_ranges:\n",
    "            tmp = copy.deepcopy(list(self.parameter_ranges[key]))\n",
    "            tmp.sort()\n",
    "            if type(self.parameter_ranges[key]) is set:\n",
    "                self.parameter_ranges[key] = set(tmp)\n",
    "            else:\n",
    "                self.parameter_ranges[key] = tuple(tmp)\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_tuning_result_df(self):\n",
    "        \"\"\" Helper to set up tuning result dataframe \"\"\"\n",
    "\n",
    "        tune_result_columns = copy.deepcopy(self.hyperparameters)\n",
    "\n",
    "        # Different set of metric columns for different types of models\n",
    "        if self.clf_type == 'Classification':\n",
    "            tune_result_columns.extend(self.classification_extra_output_columns)\n",
    "        elif self.clf_type == 'Regression':\n",
    "            tune_result_columns.extend(self.regression_extra_output_columns)\n",
    "\n",
    "        self.tuning_result = pd.DataFrame({col:list() for col in tune_result_columns})\n",
    "\n",
    "        \n",
    "    \n",
    "    def _get_checked_dict(self):\n",
    "        \"\"\" Helper to set up checked list \"\"\"\n",
    "\n",
    "        self.checked_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "    def set_non_tuneable_hyperparameters(self, non_tuneable_hyperparameter_choice):\n",
    "        \"\"\" Input Non tuneable hyperparameter choice \"\"\"\n",
    "\n",
    "        if type(non_tuneable_hyperparameter_choice) is not dict:\n",
    "            print('non_tuneable_hyeprparameters_choice must be dict, please try again')\n",
    "            return\n",
    "        \n",
    "        for nthp in non_tuneable_hyperparameter_choice:\n",
    "            if type(non_tuneable_hyperparameter_choice[nthp]) in (set, list, tuple, dict):\n",
    "                print('non_tuneable_hyperparameters_choice must not be of array-like type')\n",
    "                return\n",
    "\n",
    "        self.non_tuneable_parameter_choices = non_tuneable_hyperparameter_choice\n",
    "\n",
    "        print(\"Successfully recorded non_tuneable_hyperparameter choices\")\n",
    "\n",
    "\n",
    "\n",
    "    def read_in_transform(self, transform_update):\n",
    "        \"\"\" Function to read in transformation settings \"\"\"\n",
    "\n",
    "        if type(transform_update) is not dict:\n",
    "            print('transform_update should be a dict, please re-enter')\n",
    "            return\n",
    "\n",
    "        for key in transform_update:\n",
    "            self.transform[key] = transform_update[key]\n",
    "\n",
    "        print('Updated transform dictionary:', self.transform)\n",
    "\n",
    "    \n",
    "\n",
    "    def read_in_categorical(self, categorical_update):\n",
    "        \"\"\" Function to read in categorical settings \"\"\"\n",
    "\n",
    "        if type(categorical_update) is not list:\n",
    "            print('categorical_update should be a list, please re-enter')\n",
    "            return\n",
    "\n",
    "        for key in categorical_update:\n",
    "            self.categorical[key] = True\n",
    "            \n",
    "            self.parameter_ranges[key] = {'values': tuple(self.parameter_ranges[key])}\n",
    "        \n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        print('Updated categorical dictionary:', self.categorical)\n",
    "        print('Updated original bounds dict:', self.original_bounds)\n",
    "\n",
    "    \n",
    "\n",
    "    def set_features(self, ningxiang_output):\n",
    "        \"\"\" Input features \"\"\"\n",
    "\n",
    "        if type(ningxiang_output) is not dict:\n",
    "            print(\"Please ensure NingXiang output is a dict\")\n",
    "            return\n",
    "        \n",
    "        if not self.hyperparameters:\n",
    "            print(\"Missing hyperparameter choices, please run .set_hyperparameters() first\")\n",
    "            return\n",
    "        \n",
    "        # sort ningxiang just for safety, and store up\n",
    "        ningxiang_output_sorted = self._sort_features(ningxiang_output)\n",
    "        self.feature_n_ningxiang_score_dict = ningxiang_output_sorted\n",
    "\n",
    "        # activate this switch\n",
    "        self._tune_features = True\n",
    "\n",
    "        # update previous internal structures based on first set of hyperparameter choices\n",
    "        ##here used numbers instead of tuples as the values in parameter_choices; thus need another mapping to get map back to the features\n",
    "        self.parameter_choices['features'] = set([i for i in range(len(ningxiang_output_sorted))])\n",
    "        self.feature_combo_n_index_map = {i: list(ningxiang_output_sorted.keys())[i] for i in range(len(ningxiang_output_sorted))}\n",
    "\n",
    "        self.hyperparameters = list(self.parameter_ranges.keys())\n",
    "        \n",
    "        self.categorical['features'] = False\n",
    "        self.transform['features'] = False\n",
    "\n",
    "        self._get_checked_dict()\n",
    "        self._setup_tuning_result_df()\n",
    "\n",
    "        self.original_bounds = [(self.parameter_ranges[key], key) for key in self.parameter_ranges]\n",
    "\n",
    "        print(\"Successfully recorded tuneable feature combination choices and updated relevant internal structures\")\n",
    "\n",
    "\n",
    "    \n",
    "    def _sort_features(self, ningxiang_output):\n",
    "        \"\"\" Helper for sorting features based on NingXiang values (input dict output dict) \"\"\"\n",
    "\n",
    "        ningxiang_output_list = [(key, ningxiang_output[key]) for key in ningxiang_output]\n",
    "\n",
    "        ningxiang_output_list.sort(key = lambda x:x[1])\n",
    "\n",
    "        ningxiang_output_sorted = {x[0]:x[1] for x in ningxiang_output_list}\n",
    "\n",
    "        return ningxiang_output_sorted\n",
    "\n",
    "\n",
    "\n",
    "    def get_coords_from_bounds(bounds):\n",
    "        \"\"\" Function to get initial coordinates to tune \"\"\"\n",
    "\n",
    "        # Setup the initial list that is used in classic JiaXing combo getting algorithm\n",
    "        if type(bounds[0][0]) is tuple:\n",
    "            boundary_coordinates = [[bounds[0][0][i]] for i in range(2)]\n",
    "        elif type(bounds[0][0]) is set: \n",
    "            if len(bounds[0][0]) == 1:\n",
    "                boundary_coordinates = [[list(bounds[0][0])[0]]]\n",
    "            else:\n",
    "                boundary_coordinates = [[list(bounds[0][0])[0]], [list(bounds[0][0])[-1]]]\n",
    "        elif type(bounds[0][0]) is dict:\n",
    "            boundary_coordinates = [[bounds[0][0]['values'][i]] for i in range(len(bounds[0][0]['values']))]\n",
    "\n",
    "        # Second part of classic JiaXing combo getting algorithm\n",
    "        for i in range(1, len(bounds)):\n",
    "            old_boundary_coordinates = copy.deepcopy(boundary_coordinates)\n",
    "            boundary_coordinates = list()\n",
    "\n",
    "            values = bounds[i]\n",
    "\n",
    "            for init_coord in old_boundary_coordinates:\n",
    "                if type(values[0]) is tuple: # tuple: continuous values\n",
    "                    for value in values[0]:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(value)\n",
    "                        boundary_coordinates.append(tmp)\n",
    "\n",
    "                elif type(values[0]) is set: # set: semi-continuous values (ordinal or floats but not continuous) \n",
    "                    if len(values[0]) == 1:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(list(values[0])[0])\n",
    "                        boundary_coordinates.append(tmp)\n",
    "                    else:\n",
    "                        for value in [list(values[0])[0], list(values[0])[-1]]:\n",
    "                            tmp = copy.deepcopy(init_coord)\n",
    "                            tmp.append(value)\n",
    "                            boundary_coordinates.append(tmp)\n",
    "                \n",
    "                elif type(values[0]) is dict: # dict: discrete values\n",
    "                    for value in values[0]['values']:\n",
    "                        tmp = copy.deepcopy(init_coord)\n",
    "                        tmp.append(value)\n",
    "                        boundary_coordinates.append(tmp)\n",
    "\n",
    "        return boundary_coordinates\n",
    "\n",
    "\n",
    "\n",
    "    def get_centre_components(self, bounds, categorical):\n",
    "        \"\"\" Helper that gets the centres of a bound as lists (considering for categorical) \"\"\"\n",
    "\n",
    "        # Classic JiaXing getting combo algorithm\n",
    "        centre_components = list()\n",
    "        tmp_cat = copy.deepcopy(categorical)\n",
    "        for i in range(len(bounds)):\n",
    "\n",
    "            if type(bounds[i][0]) is tuple: # tuple: continuous values\n",
    "                # take the mean value\n",
    "                centre_components.append(sum(bounds[i][0])/2)\n",
    "\n",
    "            elif type(bounds[i][0]) is set: # set: semi-continuous values (ordinal or floats but not continuous) \n",
    "                n_semicont_values = len(bounds[i][0])\n",
    "                if n_semicont_values <= 2:\n",
    "\n",
    "                    # just input the tuple(set) as the centre (which will be recognised as two discrete)\n",
    "                    centre_components.append(tuple(bounds[i][0]))\n",
    "\n",
    "                    # set categorical of this variable to true because no more semi-continuous values in between the current bounds\n",
    "                    tmp_cat[self.hyperparameters[i]] = True \n",
    "\n",
    "                else:\n",
    "                    # take the middle\n",
    "                    centre_components.append(list(bounds[i][0])[n_semicont_values//2])\n",
    "\n",
    "            elif type(bounds[i][0]) is dict: # dict: discrete values\n",
    "                # input the value (a set) which will be recognised as discrete\n",
    "                centre_components.append(bounds[i][0]['values'])\n",
    "        \n",
    "        # returns 1. components that can be unpacked into multiple centres; 2. new categorical labels\n",
    "        return centre_components, tmp_cat\n",
    "\n",
    "\n",
    "\n",
    "    def unpack_centre(self, centre_components):\n",
    "        \"\"\" Helper to unpack centre components into centre \"\"\"\n",
    "\n",
    "        # Classic JiaXing algorithm for getting all combinations\n",
    "        centres = [[]]\n",
    "        for i in range(len(centre_components)):\n",
    "            old_centres = copy.deepcopy(centres)\n",
    "            centres = list()\n",
    "            if type(centre_components[i]) is tuple:\n",
    "                for obj in centre_components[i]:\n",
    "                    for cent in old_centres:\n",
    "                        tmp_cent = copy.deepcopy(cent)\n",
    "                        tmp_cent.append(obj)\n",
    "                        centres.append(tmp_cent)\n",
    "            else:\n",
    "                for cent in old_centres:\n",
    "                    tmp_cent = copy.deepcopy(cent)\n",
    "                    tmp_cent.append(centre_components[i])\n",
    "                    centres.append(tmp_cent)\n",
    "\n",
    "        return [tuple(centre) for centre in centres]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_categorical(self, new_cat, boundaries):\n",
    "        \"\"\" Helper to get all combos of categorical feature's values (for use in OLS) \"\"\"\n",
    "\n",
    "        # Classic JiaXing algorithm for getting all combinations\n",
    "        out = [[]]\n",
    "        for hyperparameter in new_cat:\n",
    "            if new_cat[hyperparameter] is True:\n",
    "                old_out = copy.deepcopy(out)\n",
    "                out = list()\n",
    "\n",
    "                val_list = list(boundaries[hyperparameter])\n",
    "                val_list_unique = list(set(val_list))\n",
    "                val_list_unique.sort()\n",
    "\n",
    "                for val in val_list_unique:\n",
    "                    for lst in old_out:\n",
    "                        tmp = copy.deepcopy(lst)\n",
    "                        tmp.append(val)\n",
    "                        out.append(tmp)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def get_new_bounds(self, bounds, centre, categorical):\n",
    "        \"\"\" Function to get new bounds \"\"\"\n",
    "\n",
    "        # get the range components that make the 2^d bounds\n",
    "        range_components = self.get_range_components(bounds, centre, categorical)\n",
    "\n",
    "        # make the new bounds from components\n",
    "        new_bounds = self.make_bounds(range_components)\n",
    "\n",
    "        return new_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def get_range_components(self, bounds, centre, categorical):\n",
    "        \"\"\" Helper that gets the range components \"\"\"\n",
    "\n",
    "        range_components = list()\n",
    "        for i in range(len(bounds)):\n",
    "            \n",
    "            if type(bounds[i][0]) is tuple:\n",
    "                lower_range = (bounds[i][0][0], centre[i]) \n",
    "                upper_range = (centre[i], bounds[i][0][1]) \n",
    "\n",
    "                ranges = (lower_range, upper_range)\n",
    "\n",
    "                range_components.append((ranges, bounds[i][1]))\n",
    "            \n",
    "            elif type(bounds[i][0]) is set:\n",
    "                \n",
    "                if categorical[self.hyperparameters[i]] is False:\n",
    "                    lower_range_min_max = (list(bounds[i][0])[0], centre[i]) \n",
    "                    upper_range_min_max = (centre[i], list(bounds[i][0])[-1]) \n",
    "                    lower_range = list()\n",
    "                    for orig_val in list(self.parameter_ranges[self.hyperparameters[i]]):\n",
    "                        if orig_val <= lower_range_min_max[1] and lower_range_min_max[0] <= orig_val:\n",
    "                            lower_range.append(orig_val)\n",
    "                    lower_range.sort()\n",
    "                    lower_range = set(lower_range)\n",
    "                    \n",
    "                    upper_range = list()\n",
    "                    for orig_val in list(self.parameter_ranges[self.hyperparameters[i]]):\n",
    "                        if orig_val <= upper_range_min_max[1] and upper_range_min_max[0] <= orig_val:\n",
    "                            upper_range.append(orig_val)\n",
    "                    upper_range.sort()\n",
    "                    upper_range = set(upper_range)\n",
    "\n",
    "                    ranges = (lower_range, upper_range)\n",
    "\n",
    "                    range_components.append((ranges, bounds[i][1]))\n",
    "                \n",
    "                else:\n",
    "                    ranges = bounds[i][0]\n",
    "                    range_components.append((ranges, bounds[i][1]))\n",
    "\n",
    "\n",
    "            elif type(bounds[i][0]) is dict:\n",
    "                \n",
    "                ranges = set(bounds[i][0]['values'])\n",
    "\n",
    "                range_components.append((ranges, bounds[i][1]))\n",
    "        \n",
    "        return range_components\n",
    "\n",
    "\n",
    "\n",
    "    def make_bounds(self, range_components):\n",
    "        \"\"\" Helper that makes the bounds using range components \"\"\"\n",
    "\n",
    "        # Algorithm to create all bounds\n",
    "        if type(range_components[0][0]) is tuple:\n",
    "            bounds = [[(range_components[0][0][i], range_components[0][1])] for i in range(2)] # hardcode cos bounds can only have 2 values\n",
    "        elif type(range_components[0][0]) is set:\n",
    "            tmp_tup = tuple(range_components[0][0])\n",
    "            bounds = [[({tmp_tup[i]}, range_components[0][1])] for i in range(len(tmp_tup))]\n",
    "\n",
    "        for i in range(1, len(range_components)):\n",
    "            old_bounds = copy.deepcopy(bounds)\n",
    "            bounds = list()\n",
    "\n",
    "            values = range_components[i]\n",
    "\n",
    "            for bound in old_bounds:\n",
    "                \n",
    "                if type(values[0]) == tuple:\n",
    "                    for value in values[0]:\n",
    "                        tmp = copy.deepcopy(bound)\n",
    "                        tmp.append((value, values[1]))\n",
    "\n",
    "                        bounds.append(tmp)\n",
    "                \n",
    "                elif type(values[0]) == set:\n",
    "                    for value in list(values[0]):\n",
    "                        tmp = copy.deepcopy(bound)\n",
    "                        tmp.append(({value}, values[1]))\n",
    "\n",
    "                        bounds.append(tmp)\n",
    "        \n",
    "        return bounds\n",
    "\n",
    "\n",
    "\n",
    "    def rebuild_bounds_to_original_format(self, tmp_boundary, new_cat):\n",
    "        \"\"\" Helper to rebuild current format of bounds (as a df) into original bound format\"\"\"\n",
    "\n",
    "        tmp_boundary = tmp_boundary.drop(['score'], axis = 1)\n",
    "        \n",
    "        bounds_original_format = list()\n",
    "        for col in tmp_boundary.columns:\n",
    "\n",
    "        # if already categorical: just keep it as categorical\n",
    "            if new_cat[col] == True: \n",
    "                bounds_original_format.append(({'values': tuple(set(tmp_boundary[col]))}, col))\n",
    "        \n",
    "            else:\n",
    "                col_vals = list(set(tmp_boundary[col]))\n",
    "            \n",
    "                if type(self.parameter_ranges[col]) is set:\n",
    "                    tmp = list()\n",
    "                    curr_val_max = max(col_vals)\n",
    "                    curr_val_min = min(col_vals)\n",
    "                    for orig_val in list(self.parameter_ranges[col]):\n",
    "                        if orig_val <= curr_val_max and curr_val_min <= orig_val:\n",
    "                            tmp.append(orig_val)\n",
    "                    tmp.sort()\n",
    "                    tmp = set(tmp)\n",
    "                    bounds_original_format.append((tmp, col))\n",
    "\n",
    "                else: # continuous values\n",
    "                    bounds_original_format.append(((min(col_vals), max(col_vals)), col))\n",
    "\n",
    "        return bounds_original_format\n",
    "\n",
    "\n",
    "\n",
    "    def _get_list_from_df(self, df):\n",
    "        \"\"\" Helper to get df rows into list form \"\"\"\n",
    "        \n",
    "        out = list()\n",
    "        for row in df.iterrows():\n",
    "            out.append(list(row[1].values))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def _get_protective_bounds(self, bounds):\n",
    "        \"\"\" Helper to get protective bounds  - for boundaries\"\"\"\n",
    "\n",
    "        protective_bounds = list()\n",
    "        for bound in bounds:\n",
    "            if type(bound[0]) is dict: # categorical values become a set\n",
    "                protective_bounds.append(set(bound[0]['values']))\n",
    "            elif type(bound[0]) is set: # semi_categorical values become tuple\n",
    "                protective_bounds.append((list(bound[0])[0], list(bound[0])[-1]))\n",
    "            elif type(bound[0]) is tuple: # continuous values stay as tuple\n",
    "                protective_bounds.append(bound[0])\n",
    "        \n",
    "        return protective_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def _get_protective_bounds2(self, tmp_boundary, new_cat):\n",
    "        \"\"\" Helper to get protective bounds - for centres \"\"\"\n",
    "\n",
    "        protective_bounds = list()\n",
    "        for col in tmp_boundary.columns:\n",
    "            if col == 'score':\n",
    "                 continue\n",
    "            \n",
    "            col_values = list(tmp_boundary[col])\n",
    "\n",
    "            if new_cat[col]: # categorical values - only one left - set\n",
    "                protective_bounds.append({col_values[0],})\n",
    "            \n",
    "            else:  # continuous values - tuple\n",
    "                protective_bounds.append([min(col_values), max(col_values)])\n",
    "                 \n",
    "        \n",
    "        return protective_bounds\n",
    "    \n",
    "\n",
    "\n",
    "    def _in_protective_bounds(self, centre):\n",
    "        \"\"\" Determine whether centre is in protective_bounds \"\"\"\n",
    "\n",
    "        for i in range(len(centre)):\n",
    "            if type(self._protective_bounds[i]) is set: # categorical\n",
    "                if centre[i] not in self._protective_bounds[i]: # not matching any of the categorical values\n",
    "                    return False \n",
    "            else: # continuous values\n",
    "                if centre[i] > self._protective_bounds[i][1] or centre[i] < self._protective_bounds[i][0]: # outside of boundary tuple\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "    def _protective_bounds_to_original_bounds(self):\n",
    "        \"\"\" Helper to turn protective bounds back to original bounds \"\"\"\n",
    "\n",
    "        protective_to_original_bounds = list()\n",
    "        for i in range(len(self._protective_bounds)):\n",
    "            if type(self._protective_bounds[i]) is set: # discrete\n",
    "                protective_to_original_bounds.append(({'values': tuple(self._protective_bounds[i])}, self.original_bounds[i][1]))\n",
    "            else:\n",
    "                if type(self.original_bounds[i][0]) is set:\n",
    "                    \n",
    "                    tmp = list()\n",
    "                    for val in list(self.original_bounds[i][0]):\n",
    "                        if val <= self._protective_bounds[i][1] and val >= self._protective_bounds[i][0]:\n",
    "                            tmp.append(val)\n",
    "\n",
    "                    protective_to_original_bounds.append((set(val), self.original_bounds[i][1]))\n",
    "                else:\n",
    "                    protective_to_original_bounds.append((self._protective_bounds[i], self.original_bounds[i][1]))\n",
    "        \n",
    "        return protective_to_original_bounds\n",
    "\n",
    "\n",
    "\n",
    "    def tune(self, key_stats_only = False):\n",
    "        \"\"\" Begin tuning \"\"\"\n",
    "\n",
    "        print(\"Begin Guidance\")\n",
    "\n",
    "        self.key_stats_only = key_stats_only\n",
    "\n",
    "        self._round = 0\n",
    "\n",
    "        # start by putting original bounds into a list; this list is the object that will control whether algorithm has terminated\n",
    "        bounds_list = [self.original_bounds]\n",
    "        centre_down = [False]\n",
    "\n",
    "        while bounds_list: # gets reset every time, so algo will keep running if there are bounds to operate on\n",
    "\n",
    "            old_bounds_list = copy.deepcopy(bounds_list)\n",
    "            bounds_list = list()\n",
    "            old_centre_down = copy.deepcopy(centre_down)\n",
    "            centre_down = list()\n",
    "\n",
    "            for k in range(len(old_bounds_list)): # now run algorithm on every bound\n",
    "\n",
    "                # get the coordinates that define the bounds\n",
    "                coords_to_tune = self.get_coords_from_bounds(old_bounds_list[k]) \n",
    "\n",
    "                # get all coordinates into a DataFrame - used for getting boundary\n",
    "                boundaries = pd.DataFrame()\n",
    "                for coord in coords_to_tune:\n",
    "                    \n",
    "                    # combination that goes straight into OLS\n",
    "                    combo_OLS_dict = {self.hyperparameters[i]:[coord[i]] for i in range(len(self.hyperparameters))}\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(coord) in self.checked_dict:\n",
    "                        print(f'Already Trained and Tested a combination')\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(coord)]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        combo_dict = dict() # combination that gets transformed for searching\n",
    "                        for i in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[i]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[i]] = [10**coord[i]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[i]] = [coord[i]] \n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(combo_dict)\n",
    "                        combo_OLS_dict['score'] = [self.val_score] \n",
    "\n",
    "                        if self._is_new_best:\n",
    "                            self._protective_bounds = self._get_protective_bounds(old_bounds_list[k])\n",
    "                            # TODO: export self._protective_bounds as json\n",
    "\n",
    "                        # store its metadata into checked_dict\n",
    "                        self.checked_dict[tuple(coord)] = {'score': self.val_score, 'combo_type': 'Boundary'} #TODO: 可以移除\n",
    "\n",
    "                    # put this coord into df containing all boundaries (for later sliming depending on centre, and then OLS)\n",
    "                    tmp_boundary = pd.DataFrame(combo_OLS_dict)\n",
    "                    boundaries = boundaries.append(tmp_boundary)\n",
    "\n",
    "                # get the components that make up the centre (as well as new categories); and then unpack them into centres\n",
    "                centre_components, new_cat = self.get_centre_components(old_bounds_list[k], self.categorical) # 加进去 - 改 for bound bounds with index\n",
    "                \n",
    "                centres = self.unpack_centre(centre_components)\n",
    "\n",
    "                # get the categorical features' values into a list for use in OLS preparation\n",
    "                categorical_value_list = self.get_categorical(new_cat, boundaries)\n",
    "\n",
    "                for i in range(len(centres)): # run through each different centre\n",
    "\n",
    "                    # create a dataframe version of centre (so we could put it into OLS)\n",
    "                    centre_OLS_df = pd.DataFrame({self.hyperparameters[j]:[centres[i][j]] for j in range(len(centres[i]))})\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(centres[i]) in self.checked_dict:\n",
    "                        print(f'Already Trained and Tested a combination')\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(centres[i])]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        centre_df = dict()\n",
    "                        for j in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[j]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[j]] = [10**centres[i][j]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[j]] = [coord[j]]\n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(centre_df) \n",
    "                        actual_centre_score = self.val_score\n",
    "\n",
    "                        # store its metadata into checked_list\n",
    "                        self.checked_dict[tuple(centres[i])] = {'score': self.val_score, 'type': 'Centre'} #TODO: 可以移除\n",
    "\n",
    "                    # copy the boundary dataframes - to turn into the correct training data for OLS (one lm model for each centre)\n",
    "                    tmp_boundary = copy.deepcopy(boundaries)\n",
    "                    tmp_boundary_drop = copy.deepcopy(boundaries)\n",
    "                    \n",
    "                    n_cat = 0\n",
    "                    for j in range(len(new_cat)):\n",
    "                \n",
    "                        if new_cat[self.hyperparameters[j]] == True:\n",
    "\n",
    "                            tmp_boundary = tmp_boundary[tmp_boundary[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop[tmp_boundary_drop[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop.drop([self.hyperparameters[j]], axis = 1)\n",
    "                            centre_OLS_df = centre_OLS_df.drop([self.hyperparameters[j]], axis=1)\n",
    "                            n_cat += 1\n",
    "\n",
    "                    tmp_boundary_X = tmp_boundary_drop.drop(['score'], axis = 1)\n",
    "                    tmp_boundary_y = tmp_boundary_drop['score']\n",
    "\n",
    "                    OLS = sm.OLS(tmp_boundary_y, tmp_boundary_X).fit()\n",
    "                    pred_centre_score = OLS.predict(centre_OLS_df)[0]\n",
    "\n",
    "                    if self._is_new_best:\n",
    "                        self._protective_bounds = self._get_protective_bounds2(tmp_boundary, new_cat)\n",
    "                        # TODO: export self._protective_bounds as json\n",
    "\n",
    "\n",
    "                    if self._rounds == 0: \n",
    "                        if actual_centre_score < pred_centre_score - 0.005: # first round + down: mark 'down' and repeat\n",
    "\n",
    "                            bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                            new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "                            bounds_list.extend(new_bounds_list)\n",
    "                            \n",
    "                            # record whether centre has gone down\n",
    "                            tmp_centre_down = [True for j in range(len(new_bounds_list))]\n",
    "                            centre_down.extend(tmp_centre_down)\n",
    "                    \n",
    "                        else: # first round + down OR first round + fits: repeat\n",
    "                            bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                            new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "                            bounds_list.extend(new_bounds_list)\n",
    "                            \n",
    "                            # record whether centre has gone down\n",
    "                            tmp_centre_down = [False for j in range(len(new_bounds_list))]\n",
    "                            centre_down.extend(tmp_centre_down)\n",
    "                                \n",
    "                    \n",
    "                    else:\n",
    "                        if actual_centre_score < pred_centre_score - 0.005: # not first round, in protective zone, down: mark 'down' and repeat\n",
    "                                \n",
    "                            if self._in_protective_bounds(centres[i]):\n",
    "\n",
    "                                if old_centre_down[k] == True: # not first round, not in protective zone, double down, down: stop\n",
    "                                    pass\n",
    "\n",
    "                                else: # not first round, not in protective zone, not double down, down: mark 'down' and repeat\n",
    "                                    bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                                    new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "                                    bounds_list.extend(new_bounds_list)\n",
    "                                    \n",
    "                                    # record whether centre has gone down\n",
    "                                    tmp_centre_down = [True for j in range(len(new_bounds_list))]\n",
    "                                    centre_down.extend(tmp_centre_down)\n",
    "                        \n",
    "                        elif actual_centre_score > pred_centre_score + 0.005: # not first round, in protective zone, up: repeat\n",
    "                            bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                            new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "                            bounds_list.extend(new_bounds_list)\n",
    "                            \n",
    "                            # record whether centre has gone down\n",
    "                            tmp_centre_down = [False for j in range(len(new_bounds_list))]\n",
    "                            centre_down.extend(tmp_centre_down)\n",
    "                        \n",
    "                        else: # not first round, in protective zone, fits: stop\n",
    "                            pass\n",
    "            \n",
    "            self._round += 1\n",
    "\n",
    "\n",
    "        # Cruise algorithm\n",
    "        print(\"Begin Cruise\")\n",
    "\n",
    "        cruise_bounds = [self._protective_bounds_to_original_bounds()]\n",
    "\n",
    "        run_through = True\n",
    "\n",
    "        while cruise_bounds: # gets reset every time, so algo will keep running if there are bounds to operate on\n",
    "            \n",
    "            if run_through == True:\n",
    "                old_max_bounds = cruise_bounds[0]\n",
    "\n",
    "            old_cruise_bounds = copy.deepcopy(cruise_bounds)\n",
    "            cruise_bounds = list()\n",
    "\n",
    "            for k in range(len(old_cruise_bounds)): # now run algorithm on every bound\n",
    "\n",
    "                # get the coordinates that define the bounds\n",
    "                coords_to_tune = self.get_coords_from_bounds(old_cruise_bounds[k]) \n",
    "\n",
    "                # get all coordinates into a DataFrame - used for getting boundary\n",
    "                boundaries = pd.DataFrame()\n",
    "                for coord in coords_to_tune:\n",
    "                    \n",
    "                    # combination that goes straight into OLS\n",
    "                    combo_OLS_dict = {self.hyperparameters[i]:[coord[i]] for i in range(len(self.hyperparameters))}\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(coord) in self.checked_dict:\n",
    "                        print(f'Already Trained and Tested a combination')\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(coord)]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        combo_dict = dict() # combination that gets transformed for searching\n",
    "                        for i in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[i]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[i]] = [10**coord[i]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[i]] = [coord[i]] \n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(combo_dict)\n",
    "                        combo_OLS_dict['score'] = [self.val_score] \n",
    "\n",
    "                        if self._is_new_best:\n",
    "                            self._protective_bounds = self._get_protective_bounds(old_cruise_bounds[k])\n",
    "                            # TODO: export self._protective_bounds as json\n",
    "\n",
    "                        # store its metadata into checked_dict\n",
    "                        self.checked_dict[tuple(coord)] = {'score': self.val_score, 'combo_type': 'Boundary'} #TODO: 可以移除\n",
    "\n",
    "                    # put this coord into df containing all boundaries (for later sliming depending on centre, and then OLS)\n",
    "                    tmp_boundary = pd.DataFrame(combo_OLS_dict)\n",
    "                    boundaries = boundaries.append(tmp_boundary)\n",
    "\n",
    "                # get the components that make up the centre (as well as new categories); and then unpack them into centres\n",
    "                centre_components, new_cat = self.get_centre_components(old_cruise_bounds[k], self.categorical) # 加进去 - 改 for bound bounds with index\n",
    "                \n",
    "                centres = self.unpack_centre(centre_components)\n",
    "\n",
    "                # get the categorical features' values into a list for use in OLS preparation\n",
    "                categorical_value_list = self.get_categorical(new_cat, boundaries)\n",
    "\n",
    "                for i in range(len(centres)): # run through each different centre\n",
    "\n",
    "                    # create a dataframe version of centre (so we could put it into OLS)\n",
    "                    centre_OLS_df = pd.DataFrame({self.hyperparameters[j]:[centres[i][j]] for j in range(len(centres[i]))})\n",
    "                    \n",
    "                    # decide whether to search (criteria: has it been searched before)\n",
    "                    if tuple(centres[i]) in self.checked_dict:\n",
    "                        print(f'Already Trained and Tested a combination')\n",
    "                        combo_OLS_dict['score'] = self.checked_dict[tuple(centres[i])]['score']\n",
    "                        \n",
    "                    else:\n",
    "                        centre_df = dict()\n",
    "                        for j in range(len(self.hyperparameters)):\n",
    "                    \n",
    "                            # transform\n",
    "                            if self.transform[self.hyperparameters[j]] == '10^':\n",
    "                                combo_dict[self.hyperparameters[j]] = [10**centres[i][j]]\n",
    "                    \n",
    "                            else:\n",
    "                                combo_dict[self.hyperparameters[j]] = [coord[j]]\n",
    "\n",
    "                        # search it\n",
    "                        self._up_to += 1\n",
    "                        self._train_and_test_combo(centre_df) \n",
    "                        actual_centre_score = self.val_score\n",
    "\n",
    "                        # store its metadata into checked_list\n",
    "                        self.checked_dict[tuple(centres[i])] = {'score': self.val_score, 'type': 'Centre'} #TODO: 可以移除\n",
    "\n",
    "                    # copy the boundary dataframes - to turn into the correct training data for OLS (one lm model for each centre)\n",
    "                    tmp_boundary = copy.deepcopy(boundaries)\n",
    "                    tmp_boundary_drop = copy.deepcopy(boundaries)\n",
    "                    \n",
    "                    n_cat = 0\n",
    "                    for j in range(len(new_cat)):\n",
    "                \n",
    "                        if new_cat[self.hyperparameters[j]] == True:\n",
    "\n",
    "                            tmp_boundary = tmp_boundary[tmp_boundary[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop[tmp_boundary_drop[self.hyperparameters[j]] == categorical_value_list[i][n_cat]]\n",
    "                            tmp_boundary_drop = tmp_boundary_drop.drop([self.hyperparameters[j]], axis = 1)\n",
    "                            centre_OLS_df = centre_OLS_df.drop([self.hyperparameters[j]], axis=1)\n",
    "                            n_cat += 1\n",
    "\n",
    "                    tmp_boundary_X = tmp_boundary_drop.drop(['score'], axis = 1)\n",
    "                    tmp_boundary_y = tmp_boundary_drop['score']\n",
    "\n",
    "                    OLS = sm.OLS(tmp_boundary_y, tmp_boundary_X).fit()\n",
    "                    pred_centre_score = OLS.predict(centre_OLS_df)[0]\n",
    "\n",
    "                    if self._is_new_best:\n",
    "                        self._protective_bounds = self._get_protective_bounds2(tmp_boundary, new_cat)\n",
    "                        # TODO: export self._protective_bounds as json\n",
    "\n",
    "                    if run_through == True:\n",
    "                        # run straight through - one more round\n",
    "                        bounds_original_format = self.rebuild_bounds_to_original_format(tmp_boundary, new_cat)\n",
    "                        new_bounds_list = self.get_new_bounds(bounds_original_format, centres[i], new_cat)\n",
    "                        cruise_bounds.extend(new_bounds_list)\n",
    "\n",
    "            if run_through == False:\n",
    "                max_bounds = self._protective_bounds_to_original_bounds()\n",
    "                if max_bounds != old_max_bounds:\n",
    "                    cruise_bounds.append(max_bounds)\n",
    "\n",
    "            run_through = not run_through\n",
    "\n",
    "\n",
    "\n",
    "    def _train_and_test_combo(self, combo):\n",
    "        \"\"\" Helper to train and test each combination as part of tune() \"\"\"\n",
    "        \n",
    "        combo = tuple(combo)\n",
    "        \n",
    "        params = {self.hyperparameters[i]:self.parameter_choices[self.hyperparameters[i]][combo[i]] for i in range(len(self.hyperparameters))}\n",
    "        \n",
    "        \n",
    "        if self._tune_features == True:\n",
    "            del params['features']\n",
    "            tmp_train_x = self.train_x[list(self.feature_combo_n_index_map[combo[-1]])] \n",
    "            tmp_val_x = self.val_x[list(self.feature_combo_n_index_map[combo[-1]])]\n",
    "            tmp_test_x = self.test_x[list(self.feature_combo_n_index_map[combo[-1]])]\n",
    "\n",
    "            # add non tuneable parameters\n",
    "            for nthp in self.non_tuneable_parameter_choices:\n",
    "                params[nthp] = self.non_tuneable_parameter_choices[nthp]\n",
    "\n",
    "            # initialise object\n",
    "            clf = self.model(**params)\n",
    "\n",
    "            params['features'] = [list(self.feature_combo_n_index_map[combo[-1]])] \n",
    "            params['feature combo ningxiang score'] = self.feature_n_ningxiang_score_dict[self.feature_combo_n_index_map[combo[-1]]]\n",
    "\n",
    "        else:\n",
    "            tmp_train_x = self.train_x\n",
    "            tmp_val_x = self.val_x\n",
    "            tmp_test_x = self.test_x\n",
    "\n",
    "            # add non tuneable parameters\n",
    "            for nthp in self.non_tuneable_parameter_choices:\n",
    "                params[nthp] = self.non_tuneable_parameter_choices[nthp]\n",
    "\n",
    "            # initialise object\n",
    "            clf = self.model(**params)\n",
    "\n",
    "        # get time and fit\n",
    "        start = time.time()\n",
    "        clf.fit(tmp_train_x, self.train_y)\n",
    "        end = time.time()\n",
    "\n",
    "        # get predicted labels/values for three datasets\n",
    "        train_pred = clf.predict(tmp_train_x)\n",
    "        val_pred = clf.predict(tmp_val_x)\n",
    "        test_pred = clf.predict(tmp_test_x)\n",
    "\n",
    "        # get scores and time used\n",
    "        time_used = end-start\n",
    "\n",
    "        # build output dictionary and save result\n",
    "        df_building_dict = params\n",
    "        for nthp in self.non_tuneable_parameter_choices:\n",
    "            del params[nthp] \n",
    "\n",
    "\n",
    "        if self.clf_type == 'Regression':\n",
    "\n",
    "            train_score = val_score = test_score = train_rmse = val_rmse = test_rmse = train_mape = val_mape = test_mape = 0\n",
    "\n",
    "            try:\n",
    "                train_score = r2_score(self.train_y, train_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_score = r2_score(self.val_y, val_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_score = r2_score(self.test_y, test_pred)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                train_rmse = np.sqrt(mean_squared_error(self.train_y, train_pred))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_rmse = np.sqrt(mean_squared_error(self.val_y, val_pred))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_rmse = np.sqrt(mean_squared_error(self.test_y, test_pred))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if self.key_stats_only == False:\n",
    "                try:\n",
    "                    train_mape = mean_absolute_percentage_error(self.train_y, train_pred)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    val_mape = mean_absolute_percentage_error(self.val_y, val_pred)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    test_mape = mean_absolute_percentage_error(self.test_y, test_pred)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            df_building_dict['Train r2'] = [np.round(train_score, 6)]\n",
    "            df_building_dict['Val r2'] = [np.round(val_score, 6)]\n",
    "            df_building_dict['Test r2'] = [np.round(test_score, 6)]\n",
    "            df_building_dict['Train RMSE'] = [np.round(train_rmse, 6)]\n",
    "            df_building_dict['Val RMSE'] = [np.round(val_rmse, 6)]\n",
    "            df_building_dict['Test RMSE'] = [np.round(test_rmse, 6)]\n",
    "            \n",
    "            if self.key_stats_only == False:\n",
    "                df_building_dict['Train MAPE'] = [np.round(train_mape, 6)]\n",
    "                df_building_dict['Val MAPE'] = [np.round(val_mape, 6)]\n",
    "                df_building_dict['Test MAPE'] = [np.round(test_mape, 6)]\n",
    "\n",
    "        \n",
    "        elif self.clf_type == 'Classification':\n",
    "\n",
    "            train_score = val_score = test_score = train_bal_accu = val_bal_accu = test_bal_accu = train_f1 = val_f1 = test_f1 = \\\n",
    "                train_precision = val_precision = test_precision = train_recall = val_recall = test_recall = 0\n",
    "\n",
    "            try:    \n",
    "                train_score = accuracy_score(self.train_y, train_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_score = accuracy_score(self.val_y, val_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_score = accuracy_score(self.test_y, test_pred)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                train_bal_accu = balanced_accuracy_score(self.train_y, train_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_bal_accu = balanced_accuracy_score(self.val_y, val_pred)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_bal_accu = balanced_accuracy_score(self.test_y, test_pred)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                train_f1 = f1_score(self.train_y, train_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_f1 = f1_score(self.val_y, val_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_f1 = f1_score(self.test_y, test_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                train_precision = precision_score(self.train_y, train_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_precision = precision_score(self.val_y, val_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_precision = precision_score(self.test_y, test_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                train_recall = recall_score(self.train_y, train_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                val_recall = recall_score(self.val_y, val_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                test_recall = recall_score(self.test_y, test_pred, average='weighted')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            df_building_dict['Train accu'] = [np.round(train_score, 6)]\n",
    "            df_building_dict['Val accu'] = [np.round(val_score, 6)]\n",
    "            df_building_dict['Test accu'] = [np.round(test_score, 6)]\n",
    "            df_building_dict['Train balanced_accuracy'] = [np.round(train_bal_accu, 6)]\n",
    "            df_building_dict['Val balanced_accuracy'] = [np.round(val_bal_accu, 6)]\n",
    "            df_building_dict['Test balanced_accuracy'] = [np.round(test_bal_accu, 6)]\n",
    "            df_building_dict['Train f1'] = [np.round(train_f1, 6)]\n",
    "            df_building_dict['Val f1'] = [np.round(val_f1, 6)]\n",
    "            df_building_dict['Test f1'] = [np.round(test_f1, 6)]\n",
    "            df_building_dict['Train precision'] = [np.round(train_precision, 6)]\n",
    "            df_building_dict['Val precision'] = [np.round(val_precision, 6)]\n",
    "            df_building_dict['Test precision'] = [np.round(test_precision, 6)]\n",
    "            df_building_dict['Train recall'] = [np.round(train_recall, 6)]\n",
    "            df_building_dict['Val recall'] = [np.round(val_recall, 6)]\n",
    "            df_building_dict['Test recall'] = [np.round(test_recall, 6)]\n",
    "\n",
    "\n",
    "        df_building_dict['Time'] = [np.round(time_used, 2)]\n",
    "\n",
    "\n",
    "        tmp = pd.DataFrame(df_building_dict)\n",
    "\n",
    "        self.tuning_result = self.tuning_result.append(tmp)\n",
    "        self._save_tuning_result()\n",
    "\n",
    "        self._is_new_best = 0\n",
    "\n",
    "        # update best score stats\n",
    "        if val_score > self.best_score: \n",
    "            self.best_score = val_score\n",
    "            self.best_clf = clf\n",
    "            self.best_combo = combo\n",
    "\n",
    "            self._is_new_best = 1\n",
    "\n",
    "        # update internal governing DataFrames\n",
    "        self.checked[combo] = 1\n",
    "        self.result[combo] = val_score\n",
    "\n",
    "        # add a new self variable compared to previous JiaXing classes\n",
    "        self.val_score = val_score\n",
    "\n",
    "        print(f'''Trained and Tested combination {self._up_to}: {combo}, taking {time_used} seconds to get val score of {val_score}\n",
    "        Current best combo: {self.best_combo} with val score {self.best_score}''')\n",
    "\n",
    "\n",
    "\n",
    "    def _save_tuning_result(self):\n",
    "        \"\"\" Helper to export tuning result csv \"\"\"\n",
    "\n",
    "        tuning_result_saving_address_strip = self.tuning_result_saving_address.split('.csv')[0]\n",
    "\n",
    "        self.tuning_result.to_csv(f'{tuning_result_saving_address_strip}.csv', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def view_best_combo_and_score(self):\n",
    "        \"\"\" View best combination and its validation score \"\"\"\n",
    "        \n",
    "        print(f'(Current) Best combo: {self.best_combo} with val score {self.best_score}')\n",
    "\n",
    "    \n",
    "\n",
    "    def read_in_tuning_result_df(self, address): \n",
    "        \"\"\" Read in tuning result csv and read data into checked and result arrays \"\"\"\n",
    "\n",
    "        if self.parameter_choices is None:\n",
    "            print(\"Missing parameter_choices to build parameter_value_map_index, please run set_hyperparameters() first\")\n",
    "\n",
    "        if self.clf_type is None:\n",
    "            print('Missing clf_type. Please run .read_in_model() first.')\n",
    "\n",
    "        self.tuning_result = pd.read_csv(address)\n",
    "        print(f\"Successfully read in tuning result of {len(self.tuning_result)} rows\")\n",
    "        \n",
    "        self._up_to = 0\n",
    "\n",
    "        # read DataFrame data into internal governing DataFrames of GuangAn\n",
    "        for row in self.tuning_result.iterrows():\n",
    "            \n",
    "            self._up_to += 1\n",
    "\n",
    "            combo = tuple([row[1][hyperparam] for hyperparam in self.hyperparameters])\n",
    "            \n",
    "            if self.clf_type == 'Regression':\n",
    "                self.checked[combo] = {'score': row[1]['Val r2']}\n",
    "            elif self.clf_type == 'Classification':\n",
    "                self.checked[combo] = {'score': row[1]['Val r2']}\n",
    "        \n",
    "            # update best score stats\n",
    "            if self.checked[combo]['score'] > self.best_score: \n",
    "                self.best_score = self.checked[combo]['score']\n",
    "                self.best_clf = None\n",
    "                print(f\"As new Best Combo {combo} is read in, best_clf is set to None\")\n",
    "                self.best_combo = combo\n",
    "    \n",
    "\n",
    "\n",
    "    def set_tuning_result_saving_address(self, address):\n",
    "        \"\"\" Read in where to save tuning object \"\"\"\n",
    "\n",
    "        self.tuning_result_saving_address = address\n",
    "        print('Successfully set tuning output address')\n",
    "\n",
    "\n",
    "    \n",
    "    def _set_object_saving_address(self, address):\n",
    "        \"\"\" Read in where to save the GuangAn object \"\"\"\n",
    "\n",
    "        self.object_saving_address = address\n",
    "        print('Successfully set object output address')\n",
    "\n",
    "\n",
    "\n",
    "    def export_guangan(self, address):\n",
    "        \"\"\" Export GuangAn object \"\"\"\n",
    "\n",
    "        self._set_object_saving_address(address)\n",
    "\n",
    "        # copy object and set big objects to None\n",
    "        object_save = copy.deepcopy(self)\n",
    "        \n",
    "        object_save.train_x = None\n",
    "        object_save.train_y = None\n",
    "        object_save.val_x = None\n",
    "        object_save.val_y = None\n",
    "        object_save.test_x = None\n",
    "        object_save.test_y = None\n",
    "        object_save._up_to = 0\n",
    "\n",
    "        # Export\n",
    "        object_saving_address_strip = self.object_saving_address.split('.pickle')[0]\n",
    "        with open(f'{object_saving_address_strip}.pickle', 'wb') as f:\n",
    "            pickle.dump(object_save, f)\n",
    "\n",
    "        print(f'Successfully exported GuangAn object as {self.object_saving_address}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GuangAn Initialised\n"
     ]
    }
   ],
   "source": [
    "guangan = GuangAn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_ranges = {\n",
    "    'A': (0, 10),\n",
    "    'B': (-4, 2),\n",
    "    'C': {0, 1, 2, 3, 4},\n",
    "    'D': {'a', 'b'}\n",
    "}\n",
    "# parameter_ranges = {\n",
    "#     'C': {0, 1, 2, 3, 4},\n",
    "#     'D': {'a', 'b'},\n",
    "#     'A': (0, 10),\n",
    "#     'B': (-4, 2),\n",
    "# }\n",
    "# parameter_ranges = {\n",
    "#     'D': {'a', 'b'},\n",
    "#     'A': (0, 10),\n",
    "#     'B': (-4, 2),\n",
    "#     'C': {0, 1, 2, 3, 4},\n",
    "# }\n",
    "\n",
    "categorical = ['D'] # 自动化？\n",
    "transform = {'B': '10^'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guangan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_18715/799759979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mguangan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_ranges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'guangan' is not defined"
     ]
    }
   ],
   "source": [
    "guangan.set_hyperparameters(parameter_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated categorical dictionary: {'D': True, 'A': False, 'B': False, 'C': False}\n",
      "Updated original bounds dict: [({'values': ('a', 'b')}, 'D'), ((0, 10), 'A'), ((-4, 2), 'B'), ({0, 1, 2, 3, 4}, 'C')]\n"
     ]
    }
   ],
   "source": [
    "guangan.read_in_categorical(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated transform dictionary: {'D': False, 'A': False, 'B': '10^', 'C': False}\n"
     ]
    }
   ],
   "source": [
    "guangan.read_in_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': {'values': ('a', 'b')}, 'A': (0, 10), 'B': (-4, 2), 'C': {0, 1, 2, 3, 4}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guangan.parameter_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
